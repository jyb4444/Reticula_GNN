{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "614129c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd185584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading required package: S4Vectors\n",
       "Loading required package: stats4\n",
       "Loading required package: BiocGenerics\n",
       "\n",
       "Attaching package: ‘BiocGenerics’\n",
       "\n",
       "The following objects are masked from ‘package:stats’:\n",
       "\n",
       "    IQR, mad, sd, var, xtabs\n",
       "\n",
       "The following objects are masked from ‘package:base’:\n",
       "\n",
       "    anyDuplicated, aperm, append, as.data.frame, basename, cbind,\n",
       "    colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,\n",
       "    get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,\n",
       "    match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,\n",
       "    Position, rank, rbind, Reduce, rownames, sapply, setdiff, sort,\n",
       "    table, tapply, union, unique, unsplit, which.max, which.min\n",
       "\n",
       "\n",
       "Attaching package: ‘S4Vectors’\n",
       "\n",
       "The following object is masked from ‘package:utils’:\n",
       "\n",
       "    findMatches\n",
       "\n",
       "The following objects are masked from ‘package:base’:\n",
       "\n",
       "    expand.grid, I, unname\n",
       "\n",
       "Loading required package: IRanges\n",
       "Loading required package: GenomicRanges\n",
       "Loading required package: GenomeInfoDb\n",
       "Loading required package: SummarizedExperiment\n",
       "Loading required package: MatrixGenerics\n",
       "Loading required package: matrixStats\n",
       "\n",
       "Attaching package: ‘MatrixGenerics’\n",
       "\n",
       "The following objects are masked from ‘package:matrixStats’:\n",
       "\n",
       "    colAlls, colAnyNAs, colAnys, colAvgsPerRowSet, colCollapse,\n",
       "    colCounts, colCummaxs, colCummins, colCumprods, colCumsums,\n",
       "    colDiffs, colIQRDiffs, colIQRs, colLogSumExps, colMadDiffs,\n",
       "    colMads, colMaxs, colMeans2, colMedians, colMins, colOrderStats,\n",
       "    colProds, colQuantiles, colRanges, colRanks, colSdDiffs, colSds,\n",
       "    colSums2, colTabulates, colVarDiffs, colVars, colWeightedMads,\n",
       "    colWeightedMeans, colWeightedMedians, colWeightedSds,\n",
       "    colWeightedVars, rowAlls, rowAnyNAs, rowAnys, rowAvgsPerColSet,\n",
       "    rowCollapse, rowCounts, rowCummaxs, rowCummins, rowCumprods,\n",
       "    rowCumsums, rowDiffs, rowIQRDiffs, rowIQRs, rowLogSumExps,\n",
       "    rowMadDiffs, rowMads, rowMaxs, rowMeans2, rowMedians, rowMins,\n",
       "    rowOrderStats, rowProds, rowQuantiles, rowRanges, rowRanks,\n",
       "    rowSdDiffs, rowSds, rowSums2, rowTabulates, rowVarDiffs, rowVars,\n",
       "    rowWeightedMads, rowWeightedMeans, rowWeightedMedians,\n",
       "    rowWeightedSds, rowWeightedVars\n",
       "\n",
       "Loading required package: Biobase\n",
       "Welcome to Bioconductor\n",
       "\n",
       "    Vignettes contain introductory material; view with\n",
       "    'browseVignettes()'. To cite Bioconductor, see\n",
       "    'citation(\"Biobase\")', and for packages 'citation(\"pkgname\")'.\n",
       "\n",
       "\n",
       "Attaching package: ‘Biobase’\n",
       "\n",
       "The following object is masked from ‘package:MatrixGenerics’:\n",
       "\n",
       "    rowMedians\n",
       "\n",
       "The following objects are masked from ‘package:matrixStats’:\n",
       "\n",
       "    anyMissing, rowMedians\n",
       "\n",
       "\n",
       "Attaching package: ‘magrittr’\n",
       "\n",
       "The following object is masked from ‘package:GenomicRanges’:\n",
       "\n",
       "    subtract\n",
       "\n",
       "\n",
       "Attaching package: ‘dplyr’\n",
       "\n",
       "The following object is masked from ‘package:Biobase’:\n",
       "\n",
       "    combine\n",
       "\n",
       "The following object is masked from ‘package:matrixStats’:\n",
       "\n",
       "    count\n",
       "\n",
       "The following objects are masked from ‘package:GenomicRanges’:\n",
       "\n",
       "    intersect, setdiff, union\n",
       "\n",
       "The following object is masked from ‘package:GenomeInfoDb’:\n",
       "\n",
       "    intersect\n",
       "\n",
       "The following objects are masked from ‘package:IRanges’:\n",
       "\n",
       "    collapse, desc, intersect, setdiff, slice, union\n",
       "\n",
       "The following objects are masked from ‘package:S4Vectors’:\n",
       "\n",
       "    first, intersect, rename, setdiff, setequal, union\n",
       "\n",
       "The following objects are masked from ‘package:BiocGenerics’:\n",
       "\n",
       "    combine, intersect, setdiff, union\n",
       "\n",
       "The following objects are masked from ‘package:stats’:\n",
       "\n",
       "    filter, lag\n",
       "\n",
       "The following objects are masked from ‘package:base’:\n",
       "\n",
       "    intersect, setdiff, setequal, union\n",
       "\n",
       "Loading required package: ggplot2\n",
       "\n",
       "Attaching package: ‘plotly’\n",
       "\n",
       "The following object is masked from ‘package:ggplot2’:\n",
       "\n",
       "    last_plot\n",
       "\n",
       "The following object is masked from ‘package:IRanges’:\n",
       "\n",
       "    slice\n",
       "\n",
       "The following object is masked from ‘package:S4Vectors’:\n",
       "\n",
       "    rename\n",
       "\n",
       "The following object is masked from ‘package:stats’:\n",
       "\n",
       "    filter\n",
       "\n",
       "The following object is masked from ‘package:graphics’:\n",
       "\n",
       "    layout\n",
       "\n",
       "Loading required package: viridisLite\n",
       "pdfCluster 1.0-4\n",
       "\n",
       "Attaching package: ‘pdfCluster’\n",
       "\n",
       "The following object is masked from ‘package:plotly’:\n",
       "\n",
       "    groups\n",
       "\n",
       "The following object is masked from ‘package:dplyr’:\n",
       "\n",
       "    groups\n",
       "\n",
       "Loading required package: lattice\n",
       "\n",
       "Attaching package: ‘caret’\n",
       "\n",
       "The following objects are masked from ‘package:DescTools’:\n",
       "\n",
       "    MAE, RMSE\n",
       "\n",
       "In addition: Warning message:\n",
       "package ‘viridis’ was built under R version 4.3.2 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R\n",
    "library(DESeq2)\n",
    "library(magrittr)\n",
    "library(SummarizedExperiment)\n",
    "library(dplyr)\n",
    "library(DESeq2)\n",
    "library(plotly)\n",
    "library(ggplot2)\n",
    "library(viridis)\n",
    "library(magrittr)\n",
    "library(pheatmap)\n",
    "library(DescTools)\n",
    "library(pdfCluster)\n",
    "library(RColorBrewer)\n",
    "library(SummarizedExperiment)\n",
    "library(caret)\n",
    "library(class)\n",
    "library(htmlwidgets)\n",
    "set.seed(88888888) # maximum luck\n",
    "\n",
    "start_time <- Sys.time()\n",
    "\n",
    "IN_DIR <- \"/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/learning_curve_analysis/input/\"\n",
    "OUT_DIR <- \"/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/learning_curve_analysis/output/\"\n",
    "\n",
    "GTEx_DATA_DIR <- IN_DIR\n",
    "GTEx_DATA_FIL <- \"rse_tcdd_data.Rdata\"\n",
    "\n",
    "ensembl2rxns.df <- read.table(paste(IN_DIR,\"Ensembl2ReactomeReactions.txt\",sep=\"\"),\n",
    "                              sep=\"\\t\")\n",
    "\n",
    "load(paste(GTEx_DATA_DIR,GTEx_DATA_FIL,sep=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91d196a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 43252   383\n",
      "[1] 43252    11\n",
      "                     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11]\n",
      "ENSMUSG00000079800.2    0    0    0    0    0    0    0    0   49    49     0\n",
      "ENSMUSG00000095092.1    0    0    0    0    0    0    0    0    0     0     0\n",
      "ENSMUSG00000079192.2    0    0    0    0    0    0    0    0    0     0     0\n",
      "ENSMUSG00000079794.2    0    0    0    0    0    0    0    0    0     0    50\n",
      "ENSMUSG00000094799.1    0    0    0    0    0    0    0    0    0     0     0\n",
      "ENSMUSG00000095250.1    0    0    0    0    0   50    0    0    0     0     0\n",
      "                     [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20]\n",
      "ENSMUSG00000079800.2    50     0    50    50     0     0     0     0     0\n",
      "ENSMUSG00000095092.1     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000079192.2     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000079794.2     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000094799.1     0     0     0     0     0     0     0     0    50\n",
      "ENSMUSG00000095250.1     0     0     0     0     0     0     0     0     0\n",
      "                     [,21] [,22] [,23] [,24] [,25] [,26] [,27] [,28] [,29]\n",
      "ENSMUSG00000079800.2     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000095092.1     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000079192.2     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000079794.2     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000094799.1     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000095250.1     0     0     0     0     0     0     0     0     0\n",
      "                     [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
      "ENSMUSG00000079800.2     0    49     0     0     0    49     0     0     0\n",
      "ENSMUSG00000095092.1     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000079192.2     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000079794.2    50     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000094799.1     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000095250.1     0     0     0     0     0     0     0     0     0\n",
      "                     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47]\n",
      "ENSMUSG00000079800.2     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000095092.1     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000079192.2     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000079794.2     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000094799.1     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000095250.1     0     0     0     0     0     0     0    48     0\n",
      "                     [,48] [,49] [,50] [,51] [,52] [,53] [,54] [,55] [,56]\n",
      "ENSMUSG00000079800.2     0     0     0     0     0     0    48     3   100\n",
      "ENSMUSG00000095092.1     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000079192.2     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000079794.2     0    50     0     0     0   100     0     0     0\n",
      "ENSMUSG00000094799.1     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000095250.1     0     0     0     0     0     0     0    48    93\n",
      "                     [,57] [,58] [,59] [,60] [,61] [,62] [,63] [,64] [,65]\n",
      "ENSMUSG00000079800.2   100     0     0    96     0     0     0    50    50\n",
      "ENSMUSG00000095092.1     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000079192.2     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000079794.2   200     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000094799.1     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000095250.1    47     0     0    49     0     0     0    49     0\n",
      "                     [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
      "ENSMUSG00000079800.2   150    50     0     0     0     0     0     0   150\n",
      "ENSMUSG00000095092.1     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000079192.2     0     0     0   150     0     0     0     0     0\n",
      "ENSMUSG00000079794.2     0     0     0    50     0     0     0     0     0\n",
      "ENSMUSG00000094799.1     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000095250.1     0     0     0     0     0     0     0     0    50\n",
      "                     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83]\n",
      "ENSMUSG00000079800.2     0    50     0    50     0     0     0     0     0\n",
      "ENSMUSG00000095092.1     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000079192.2     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000079794.2     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000094799.1     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000095250.1     0     0     0     0     0     0     0     0     0\n",
      "                     [,84] [,85] [,86] [,87] [,88] [,89] [,90] [,91] [,92]\n",
      "ENSMUSG00000079800.2   100     0     0     0   250     0     0     0     0\n",
      "ENSMUSG00000095092.1     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000079192.2     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000079794.2     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000094799.1     0     0     0     0     0     0     0     0     0\n",
      "ENSMUSG00000095250.1     0     0     0     0     0    98     0   100     0\n",
      "                     [,93] [,94] [,95] [,96] [,97] [,98] [,99] [,100] [,101]\n",
      "ENSMUSG00000079800.2     0     0     0     0     0     0    50      0      0\n",
      "ENSMUSG00000095092.1     0     0     0     0     0     0     0      0      0\n",
      "ENSMUSG00000079192.2     0     0     0     0     0     0     0      0      0\n",
      "ENSMUSG00000079794.2     0     0     0     0     0     0     0      0      0\n",
      "ENSMUSG00000094799.1     0     0     0     0     0     0     0      0      0\n",
      "ENSMUSG00000095250.1     0     0     0     0     0     0     0     49      0\n",
      "                     [,102] [,103] [,104] [,105] [,106] [,107] [,108] [,109]\n",
      "ENSMUSG00000079800.2     50      0      0     34      0     50     50      0\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0      0      0      0      0     50      0      0\n",
      "                     [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117]\n",
      "ENSMUSG00000079800.2      0      0     99     50      0      0      0     50\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0     50    150      0      0      0     48      0\n",
      "                     [,118] [,119] [,120] [,121] [,122] [,123] [,124] [,125]\n",
      "ENSMUSG00000079800.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0      0      0      0      0      0      0     50\n",
      "                     [,126] [,127] [,128] [,129] [,130] [,131] [,132] [,133]\n",
      "ENSMUSG00000079800.2      0      0      0      0    150      0      0     50\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      5      0      0\n",
      "ENSMUSG00000079794.2     50      0      0      0      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0      0      0     49     98      0      0      0\n",
      "                     [,134] [,135] [,136] [,137] [,138] [,139] [,140] [,141]\n",
      "ENSMUSG00000079800.2      0      0      0      0      0      0     50     50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0      0      0     50     48      0      0      0\n",
      "                     [,142] [,143] [,144] [,145] [,146] [,147] [,148] [,149]\n",
      "ENSMUSG00000079800.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0      0      0      0      0      0      0      0\n",
      "                     [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157]\n",
      "ENSMUSG00000079800.2      0      0      0      0      0     50      0     48\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0      0      0     50     50      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0      0      0      0      0      0     98    145\n",
      "                     [,158] [,159] [,160] [,161] [,162] [,163] [,164] [,165]\n",
      "ENSMUSG00000079800.2      0     50     43      0     50      0     84      0\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1     49     50      0      0      0      0     49      0\n",
      "                     [,166] [,167] [,168] [,169] [,170] [,171] [,172] [,173]\n",
      "ENSMUSG00000079800.2      0    100      0      0      0    100     50      0\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0      0      0      0      0      0      0     50\n",
      "                     [,174] [,175] [,176] [,177] [,178] [,179] [,180] [,181]\n",
      "ENSMUSG00000079800.2      0      0      0      0      0     50     49      0\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      9      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0      0     50      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1     48     97      0      0     98     48      0     49\n",
      "                     [,182] [,183] [,184] [,185] [,186] [,187] [,188] [,189]\n",
      "ENSMUSG00000079800.2      0     49      0     50      0      0      0      0\n",
      "ENSMUSG00000095092.1      0      0     42      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0     50      0      0      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0      0      0      0      0      0     48      0\n",
      "                     [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197]\n",
      "ENSMUSG00000079800.2      0    100      0      0      0      0      0      0\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1     49      0      0      0      0     48     48      0\n",
      "                     [,198] [,199] [,200] [,201] [,202] [,203] [,204] [,205]\n",
      "ENSMUSG00000079800.2     90      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095092.1      0     50      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2     50      0     46     46      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0      0      0      0     50      0      0      0\n",
      "                     [,206] [,207] [,208] [,209] [,210] [,211] [,212] [,213]\n",
      "ENSMUSG00000079800.2      0      0      0      0      0      0      0    150\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0      0      0      0      0      4\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0      0      0      0      0      0      0      0\n",
      "                     [,214] [,215] [,216] [,217] [,218] [,219] [,220] [,221]\n",
      "ENSMUSG00000079800.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0     49\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0      0      0      0      0      0      0    148\n",
      "                     [,222] [,223] [,224] [,225] [,226] [,227] [,228] [,229]\n",
      "ENSMUSG00000079800.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0      0      0      0      0      0      0      0\n",
      "                     [,230] [,231] [,232] [,233] [,234] [,235] [,236] [,237]\n",
      "ENSMUSG00000079800.2      0      0      0      0     50      0      0      0\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0      0      0      0      0      0      0      0\n",
      "                     [,238] [,239] [,240] [,241] [,242] [,243] [,244] [,245]\n",
      "ENSMUSG00000079800.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0      0      0      0      0     50\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0      0      0      0      0      0      0      0\n",
      "                     [,246] [,247] [,248] [,249] [,250] [,251] [,252] [,253]\n",
      "ENSMUSG00000079800.2      0      0      0      0      0     50      0      0\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0      0      0      0      0      0      0      0\n",
      "                     [,254] [,255] [,256] [,257] [,258] [,259] [,260] [,261]\n",
      "ENSMUSG00000079800.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0     50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0      0      0      0      0      0      0      0\n",
      "                     [,262] [,263] [,264] [,265] [,266] [,267] [,268] [,269]\n",
      "ENSMUSG00000079800.2      0      0      0      0      0    100    150     50\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0      0      0      0      0      0      0      0\n",
      "                     [,270] [,271] [,272] [,273] [,274] [,275] [,276] [,277]\n",
      "ENSMUSG00000079800.2      0      0     50      0     50     40     34     49\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0    100      0      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0      0      0      0     49      0      0      0\n",
      "                     [,278] [,279] [,280] [,281] [,282] [,283] [,284] [,285]\n",
      "ENSMUSG00000079800.2     50     38     50    149      0      0     50      0\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1     49      0     49      0      0     48      0      0\n",
      "                     [,286] [,287] [,288] [,289] [,290] [,291] [,292] [,293]\n",
      "ENSMUSG00000079800.2      0      0      0      0      0      0      0     50\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2     50      0      0      0      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0      0      0      0      0      0      0      0\n",
      "                     [,294] [,295] [,296] [,297] [,298] [,299] [,300] [,301]\n",
      "ENSMUSG00000079800.2     50     50     50     50      0      0      0      0\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      6      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1     50      0      0     16      0      0      0      0\n",
      "                     [,302] [,303] [,304] [,305] [,306] [,307] [,308] [,309]\n",
      "ENSMUSG00000079800.2     98     50     49      0      0     82    150      0\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0    150      0     67     50      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1     49      0      0      0     99      0      0     48\n",
      "                     [,310] [,311] [,312] [,313] [,314] [,315] [,316] [,317]\n",
      "ENSMUSG00000079800.2      0      0     50    100      0     44      0    200\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0     50      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0     50      0     50      0     50\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0      0     50      0      0      0     50      0\n",
      "                     [,318] [,319] [,320] [,321] [,322] [,323] [,324] [,325]\n",
      "ENSMUSG00000079800.2    148     99    100     25    149     48     79    147\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0     50      0      0      0    100     24\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0      0      0     50      0     49     50      0\n",
      "                     [,326] [,327] [,328] [,329] [,330] [,331] [,332] [,333]\n",
      "ENSMUSG00000079800.2      0     23      0     23      0      0      0      0\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0    100      0      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0      0      0     49      0      0      0      0\n",
      "                     [,334] [,335] [,336] [,337] [,338] [,339] [,340] [,341]\n",
      "ENSMUSG00000079800.2      0      0      0     50      0      0     50     50\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0      0      0      0     50     50\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0     50      0     48      0      0      0      0\n",
      "                     [,342] [,343] [,344] [,345] [,346] [,347] [,348] [,349]\n",
      "ENSMUSG00000079800.2      0    100    100      0     50      0      0      0\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0     49      0      0      0     49     49      0\n",
      "                     [,350] [,351] [,352] [,353] [,354] [,355] [,356] [,357]\n",
      "ENSMUSG00000079800.2      0      0      0      0     50      0     50      0\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1     92      0     48      0     50      0     46      0\n",
      "                     [,358] [,359] [,360] [,361] [,362] [,363] [,364] [,365]\n",
      "ENSMUSG00000079800.2     25      0      0     49     49      0    100    150\n",
      "ENSMUSG00000095092.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000079192.2      0     17      0      0      0      0      0      0\n",
      "ENSMUSG00000079794.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0      0      0      0      0     49      0      0\n",
      "                     [,366] [,367] [,368] [,369] [,370] [,371] [,372] [,373]\n",
      "ENSMUSG00000079800.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095092.1      0      0      0      0     49      0      0      0\n",
      "ENSMUSG00000079192.2      0      0      0      0      0      0      0     30\n",
      "ENSMUSG00000079794.2      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000094799.1      0      0      0      0      0      0      0      0\n",
      "ENSMUSG00000095250.1      0      0      0     47      0      0      0      0\n",
      "                     [,374] [,375] [,376] [,377] [,378] [,379] [,380] [,381]\n",
      "ENSMUSG00000079800.2    150     50      0    100     98     50      0      0\n",
      "ENSMUSG00000095092.1      0      0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "variances <- apply(assay(final_result), 1, var)\n",
    "\n",
    "zero_variance_rows <- which(variances == 0)\n",
    "\n",
    "if (length(zero_variance_rows) > 0) {\n",
    "    deleted_data <- assay(final_result)[zero_variance_rows, , drop = FALSE]\n",
    "    save(deleted_data, file=paste0(OUT_DIR, \"deleted_data_90.RData\"))\n",
    "    new_assays <- assay(final_result)[-zero_variance_rows, , drop = FALSE]\n",
    "    new_row_data <- rowData(final_result)[-zero_variance_rows, , drop = FALSE]\n",
    "\n",
    "    new_final_result <- SummarizedExperiment(\n",
    "        assays = SimpleList(counts = new_assays),\n",
    "        rowData = new_row_data,\n",
    "        colData = colData(final_result)\n",
    "    )\n",
    "    \n",
    "    final_result <- new_final_result\n",
    "    \n",
    "    print(dim(assay(final_result)))\n",
    "    print(dim(rowData(final_result)))\n",
    "} else {\n",
    "    cat(\"No rows with zero variance found.\\n\")\n",
    "}\n",
    "\n",
    "print(head(assay(final_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b149e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 43252   136\n",
      "[1] 136 198\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "library(tibble)\n",
    "\n",
    "keep_samples <- colData(final_result)$dose %in% c(0.00, 30.00)\n",
    "\n",
    "final_result <- final_result[, keep_samples]\n",
    "\n",
    "temp_df <- as_tibble(colData(final_result)) %>%\n",
    "  filter(!grepl(\"SRP131784\", study))\n",
    "\n",
    "tcdd_data <- temp_df\n",
    "final_result <- final_result[, colData(final_result)$external_id %in% temp_df$external_id]\n",
    "\n",
    "print(dim(final_result))\n",
    "print(dim(tcdd_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a7176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Iteration 1 dimensions:\"\n",
      "[1] 122 198\n",
      "[1] 43252   122\n",
      "[1] \"Iteration 2 dimensions:\"\n",
      "[1] 122 198\n",
      "[1] 43252   122\n",
      "[1] \"Iteration 3 dimensions:\"\n",
      "[1] 122 198\n",
      "[1] 43252   122\n",
      "[1] \"Iteration 4 dimensions:\"\n",
      "[1] 122 198\n",
      "[1] 43252   122\n",
      "[1] \"Iteration 5 dimensions:\"\n",
      "[1] 122 198\n",
      "[1] 43252   122\n",
      "[1] \"Iteration 6 dimensions:\"\n",
      "[1] 122 198\n",
      "[1] 43252   122\n",
      "[1] \"Iteration 7 dimensions:\"\n",
      "[1] 122 198\n",
      "[1] 43252   122\n",
      "[1] \"Iteration 8 dimensions:\"\n",
      "[1] 122 198\n",
      "[1] 43252   122\n",
      "[1] \"Iteration 9 dimensions:\"\n",
      "[1] 122 198\n",
      "[1] 43252   122\n",
      "[1] \"Iteration 10 dimensions:\"\n",
      "[1] 122 198\n",
      "[1] 43252   122\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "sampling_percentage <- 0.90\n",
    "\n",
    "tcdd_data_dose0 <- filter(tcdd_data, dose == 0)\n",
    "tcdd_data_dose30 <- filter(tcdd_data, dose == 30)\n",
    "\n",
    "final_results_list <- list()\n",
    "\n",
    "for (i in 1:10) {\n",
    "  min_count <- min(nrow(tcdd_data_dose0), nrow(tcdd_data_dose30)) * sampling_percentage\n",
    "  min_count <- floor(min_count)  \n",
    "\n",
    "  sampled_tcdd_dose0 <- sample_n(tcdd_data_dose0, size = min_count)\n",
    "  sampled_tcdd_dose30 <- sample_n(tcdd_data_dose30, size = min_count)\n",
    "\n",
    "  tcdd_data <- bind_rows(sampled_tcdd_dose0, sampled_tcdd_dose30)\n",
    "\n",
    "  temp_final_result <- final_result[, colData(final_result)$external_id %in% tcdd_data$external_id]\n",
    "  matched_indices <- match(tcdd_data$external_id, colData(temp_final_result)$external_id)\n",
    "  temp_final_result <- temp_final_result[, matched_indices]\n",
    "\n",
    "  final_results_list[[i]] <- temp_final_result\n",
    "\n",
    "  print(paste(\"Iteration\", i, \"dimensions:\"))\n",
    "  print(dim(tcdd_data))\n",
    "  print(dim(temp_final_result))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee4aba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "gtex_df <- read.csv(\"/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/gtex/input/node_features2.txt\",sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df35de1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"-----------------------round=0---------------------\"\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "index = 0\n",
    "for(final_result in final_results_list){\n",
    "    print(paste(\"-----------------------round=\", index, \"---------------------\",sep=\"\"))\n",
    "    tcdd_data <- colData(final_result)\n",
    "    dose_counts <- table(tcdd_data$dose)\n",
    "    dose_counts_df <- as.data.frame(dose_counts)\n",
    "    colnames(dose_counts_df) <- c(\"Dose\", \"Count\")\n",
    "    combined_gender <- ifelse(tcdd_data$Sex != \"\", tcdd_data$Sex, tcdd_data$gender)\n",
    "    write.table(tcdd_data$project_id,file=paste(OUT_DIR,\"tcdd_project_id030_time_cross_90\", index, \".txt\",sep=\"\"), row.names = FALSE, col.names = FALSE, sep = \"\\t\")\n",
    "    write.table(combined_gender,file=paste(OUT_DIR,\"tcdd_gender030_time_cross_90\",index,\".txt\",sep=\"\"), row.names = FALSE, col.names = FALSE, sep = \"\\t\")\n",
    "    write.table(tcdd_data$external_id,file=paste(OUT_DIR,\"tcdd_sample_id030_time_cross_90\",index,\".txt\",sep=\"\"), row.names = FALSE, col.names = FALSE, sep = \"\\t\")\n",
    "    write.table(tcdd_data$dose,file=paste(OUT_DIR,\"tcdd_dose030_time_cross_90\",index,\".txt\",sep=\"\"), row.names = FALSE, col.names = FALSE, sep = \"\\t\")\n",
    "    tcdd.df <- final_result %>% SummarizedExperiment::assay() %>% as.data.frame()\n",
    "    colnames(tcdd.df) <- colData(final_result)$external_id\n",
    "    ensembl_wo_ids <- gsub(\"\\\\.[0-9]+\",\"\",rownames(tcdd.df))\n",
    "    deleted_ensembl_wo_ids <- gsub(\"\\\\.[0-9]+\",\"\",rownames(deleted_data))\n",
    "    rownames(tcdd.df) <- ensembl_wo_ids\n",
    "    rownames(deleted_data) <- deleted_ensembl_wo_ids\n",
    "    reactome_ensembl_ids <- intersect(ensembl2rxns.df$V1,ensembl_wo_ids)\n",
    "    tcdd.df <- tcdd.df[reactome_ensembl_ids,]\n",
    "\n",
    "    tcdd.dose.detail.vec <- tcdd_data$dose\n",
    "    tcdd_dose_detail.vec <- tcdd_data$dose\n",
    "    scale.factor <- (.Machine$integer.max - 1) / max(tcdd.df)\n",
    "    tcdd.df <- round(tcdd.df * scale.factor)\n",
    "    tcdd.df <- tcdd.df + 1\n",
    "    dds <- DESeq2::DESeqDataSetFromMatrix(countData = as.matrix(tcdd.df),\n",
    "                                      colData = data.frame(Sample=colnames(tcdd.df),\n",
    "                                                           Dose=tcdd.dose.detail.vec),\n",
    "                                      design = ~ Dose)\n",
    "    vst.counts <- DESeq2::vst(dds,\n",
    "                          blind = FALSE,\n",
    "                          fitType = \"local\")\n",
    "    ensembl2rxns.df <- read.table(paste(IN_DIR,\"Ensembl2ReactomeReactions.txt\",sep=\"\"),\n",
    "                              sep=\"\\t\")\n",
    "    rxn2ensembls.nls <- list()\n",
    "    rxns_w_tcdd_ensembls.df <- ensembl2rxns.df %>% dplyr::filter(V1 %in% reactome_ensembl_ids)\n",
    "    rxns_w_tcdd_ensembls.df$V1 <- as.character(rxns_w_tcdd_ensembls.df$V1)\n",
    "    rxns_w_tcdd_ensembls.df$V2 <- as.character(rxns_w_tcdd_ensembls.df$V2)\n",
    "\n",
    "\n",
    "    for(i in 1:nrow(rxns_w_tcdd_ensembls.df)){\n",
    "      ens_id <- rxns_w_tcdd_ensembls.df$V1[i]\n",
    "      rxn_id <- rxns_w_tcdd_ensembls.df$V2[i]\n",
    "    #     print(names(rxn2ensembls.nls))\n",
    "\n",
    "      ensembl_list_for_rxn_id <- rxn2ensembls.nls[[rxn_id]]\n",
    "      if(is.null(ensembl_list_for_rxn_id)){\n",
    "    #     print(\"null\")\n",
    "        ensembl_list_for_rxn_id <- c(ens_id)\n",
    "      }\n",
    "      rxn2ensembls.nls[[rxn_id]] <- c(ensembl_list_for_rxn_id,ens_id) %>% unique()\n",
    "    }\n",
    "    \n",
    "    N_FOLDS <- 10\n",
    "    sample <- colData(vst.counts)\n",
    "    vst.count.mtx <-\n",
    "    vst.counts %>% SummarizedExperiment::assay() %>% as.data.frame()\n",
    "    rxns <- rxn2ensembls.nls %>% names()\n",
    "    rxn_knn_misclass_rate.nls <- list()\n",
    "    rxn_knn_ari.nls <- list()\n",
    "    rxn_knn_ecount.nls <- list()\n",
    "    rxn_pca.nls <- list()\n",
    "    count <- 0\n",
    "    toi_indices <- seq(1,length(tcdd_dose_detail.vec))\n",
    "    tcdd_dose_detail_vec_tis_of_interest <-\n",
    "    tcdd_dose_detail.vec[toi_indices]\n",
    "    vst.count.mtx.tis_of_interest <- vst.count.mtx[, toi_indices]\n",
    "    training_indices <-\n",
    "       caret::createDataPartition(\n",
    "          tcdd_dose_detail_vec_tis_of_interest,\n",
    "          times = 1,\n",
    "          p = 1.0, # no data will be held out when set to \"1.0\"\n",
    "          list = FALSE\n",
    "       )\n",
    "    vst.count.mtx.train <-\n",
    "    vst.count.mtx.tis_of_interest[, training_indices] #9/10ths of data\n",
    "    vst.count.mtx.test  <-\n",
    "    vst.count.mtx.tis_of_interest[, -training_indices] #1/10th of data\n",
    "    tcdd_dose_detail.vec.train <- tcdd_dose_detail_vec_tis_of_interest[training_indices]\n",
    "    \n",
    "    Y <- tcdd_dose_detail.vec.train\n",
    "    X <- readRDS(paste(OUT_DIR, \"rxn_pca_nls030_time_cross_90\",index,\".Rds\", sep = \"\"))\n",
    "    E <- read.table(paste(IN_DIR,\"ReactionNetwork_Rel.txt\",sep=\"\"))\n",
    "    rxn2nodeLabel.nls <- list()\n",
    "    nodeLabel2rxn.nls <- list()\n",
    "    for(i in 1:length(X)){\n",
    "      rxn2nodeLabel.nls[[names(X)[i]]] <- i\n",
    "      nodeLabel2rxn.nls[[i]] <- names(X)[i]\n",
    "    }\n",
    "    E <- E %>%\n",
    "    dplyr::filter(V1 %in% names(rxn2nodeLabel.nls))\n",
    "    E <- E %>%\n",
    "    dplyr::filter(V3 %in% names(rxn2nodeLabel.nls))\n",
    "    E <- E %>%\n",
    "    dplyr::filter(V1 %in% names(rxn2nodeLabel.nls)) %>%\n",
    "    dplyr::filter(V3 %in% names(rxn2nodeLabel.nls)) %>%\n",
    "    dplyr::select(V1,V3)\n",
    "    node1 <- numeric()\n",
    "    node2 <- numeric()\n",
    "    for(i in 1:nrow(E)){\n",
    "      node1 <- c(node1,rxn2nodeLabel.nls[[as.character(E$V1[i])]])\n",
    "      node2 <- c(node2,rxn2nodeLabel.nls[[as.character(E$V3[i])]])\n",
    "    }\n",
    "    z <- unlist(rxn2nodeLabel.nls)\n",
    "    y <- unlist(nodeLabel2rxn.nls)\n",
    "    E <- data.frame(node1 = node1,\n",
    "                node2 = node2)\n",
    "    X <- as.data.frame(X)\n",
    "    Y <- as.data.frame(Y)\n",
    "    missing_in_gtex <- setdiff(names(gtex_df), names(X))\n",
    "    missing_pathway <- data.frame(missing_pathway = missing_in_gtex)\n",
    "    all_values <- as.vector(as.matrix(X))\n",
    "    median_value <- median(all_values, na.rm = TRUE)\n",
    "    for (col in missing_pathway$missing_pathway) {\n",
    "      X[[col]] <- median_value\n",
    "    }\n",
    "\n",
    "    write.table(X,\n",
    "          file=paste(IN_DIR,\"node_features030_2_time_cross_90\", index, \".txt\",sep=\"\"),\n",
    "          row.names = FALSE,\n",
    "          col.names = FALSE)\n",
    "    write.table(Y,\n",
    "              file=paste(IN_DIR,\"graph_targets030_time_cross_90\", index, \".txt\",sep=\"\"),\n",
    "              row.names = FALSE,\n",
    "              col.names = FALSE)\n",
    "    index <- index + 1\n",
    "}\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db7438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import random\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn.functional as nn_func\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GraphConv, global_mean_pool\n",
    "\n",
    "INPUT_CHANNELS = 1\n",
    "OUTPUT_CHANNELS = 26\n",
    "NEW_CHANNELS = 2\n",
    "HIDDEN_CHANNELS = 64\n",
    "BATCH_SIZE = 64\n",
    "BENCHMARKING = False\n",
    "EPOCHS = 500\n",
    "\n",
    "random.seed = 88888888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedd3633",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.conv1 = GraphConv(INPUT_CHANNELS, hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, OUTPUT_CHANNELS)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_weight=None):\n",
    "        # 1. Obtain node embeddings\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        #x = nn_func.dropout(x, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def read_reactome_graph(e_fn):\n",
    "    e_v1 = []\n",
    "    e_v2 = []\n",
    "\n",
    "    for line in open(e_fn, 'r'):\n",
    "        dt = line.split()\n",
    "        node1 = int(dt[0]) - 1  # subtracting to convert R idx to python idx\n",
    "        node2 = int(dt[1]) - 1  # \" \"\n",
    "        e_v1.append(node1)\n",
    "        e_v2.append(node2)\n",
    "\n",
    "    return e_v1, e_v2\n",
    "\n",
    "\n",
    "def build_reactome_graph_datalist(e_v1, e_v2, n_fn, g_fn, pid_fn, sid_fn, gen_fn, dose_fn):\n",
    "    edge_index = torch.tensor([e_v1, e_v2], dtype=torch.long)\n",
    "    feature_v = numpy.loadtxt(n_fn)\n",
    "    target_v = numpy.loadtxt(g_fn, dtype=float, delimiter=\",\")\n",
    "    projectID_v = numpy.loadtxt(pid_fn, dtype=str, delimiter=\"\\t\")\n",
    "    sampleID_v = numpy.loadtxt(sid_fn, dtype=str, delimiter=\"\\t\")\n",
    "    gender_v = numpy.loadtxt(gen_fn, dtype=str, delimiter=\"\\t\")\n",
    "    dose_v = numpy.loadtxt(dose_fn, dtype=str, delimiter=\"\\t\")\n",
    "    \n",
    "    binary_labels = (target_v > 0).astype(int)\n",
    "    \n",
    "    print(\"labels check:\")\n",
    "    for dose, label in zip(target_v[:10], binary_labels[:10]): \n",
    "        print(f\"dose: {dose}, label: {label}\")\n",
    "\n",
    "\n",
    "    d_list = []\n",
    "    for row_idx in range(len(feature_v)):\n",
    "        features = feature_v[row_idx, :]\n",
    "        x = torch.tensor(features, dtype=torch.float)\n",
    "        x = x.unsqueeze(1)\n",
    "#         y = torch.tensor([target_v[row_idx]])\n",
    "        y = torch.tensor([binary_labels[row_idx]], dtype=torch.long)\n",
    "        \n",
    "        pid = projectID_v[row_idx]\n",
    "        sid = sampleID_v[row_idx]\n",
    "        gen = gender_v[row_idx]\n",
    "        dose = dose_v[row_idx]\n",
    "        \n",
    "        d_list.append(Data(x=x, y=y, pid=pid, sid=sid, gen=gen, dose=dose, edge_index=edge_index))\n",
    "\n",
    "    return d_list\n",
    "\n",
    "\n",
    "def build_reactome_graph_loader(d_list, batch_size):\n",
    "    loader = DataLoader(d_list, batch_size=batch_size, shuffle=False)  # True)\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "def train(loader, dv):\n",
    "    model.train()\n",
    "\n",
    "    correct = 0\n",
    "    for batch in loader:  # Iterate in batches over the training dataset.\n",
    "        batch.validate()\n",
    "        x = batch.x.to(dv)\n",
    "        e = batch.edge_index.to(dv)\n",
    "        b = batch.batch.to(dv)\n",
    "        y = batch.y.to(dv)\n",
    "\n",
    "        out = model(x, e, b)  # Perform a single forward pass.\n",
    "        loss = criterion(out, y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == y).sum())  # Check against ground-truth labels.\n",
    "    return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "def test(loader, dv):\n",
    "    model.eval()\n",
    "\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    project_ids = []\n",
    "    sample_ids = []\n",
    "    genders = []\n",
    "    doses = []\n",
    "    confidences = []\n",
    "    for batch in loader:  # Iterate in batches over the test dataset.\n",
    "        x = batch.x.to(dv)\n",
    "        e = batch.edge_index.to(dv)\n",
    "        b = batch.batch.to(dv)\n",
    "        y = batch.y.to(dv)\n",
    "        targets += torch.Tensor.tolist(y)\n",
    "        \n",
    "        project_ids += batch.pid\n",
    "        sample_ids += batch.sid\n",
    "        genders += batch.gen\n",
    "        doses += batch.dose\n",
    "        \n",
    "        out = model(x, e, b)  # Perform a single forward pass.\n",
    "        prob = torch.softmax(out, dim=1)\n",
    "        \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        predictions += torch.Tensor.tolist(pred)\n",
    "        confidences += torch.Tensor.tolist(prob)\n",
    "        \n",
    "    num_classes = len(confidences[0])\n",
    "\n",
    "    data_to_save = []\n",
    "    for i in range(len(targets)):\n",
    "        row = [project_ids[i], sample_ids[i], genders[i], doses[i], targets[i], predictions[i]] + confidences[i]\n",
    "        data_to_save.append(row)\n",
    "    data_to_save = numpy.array(data_to_save)\n",
    "    print(data_to_save)\n",
    "    \n",
    "    fmt = ['%s', '%s', '%s', '%s', '%s', '%s'] + ['%s' for _ in range(num_classes)]\n",
    "    \n",
    "    headers = ['project_ids', 'sample_ids', 'genders', 'doses', 'target', 'prediction'] + [f'confidence_class_{i}' for i in range(num_classes)]\n",
    "    numpy.savetxt(output_fn, data_to_save, fmt='\\t'.join(fmt), delimiter='\\t', header='\\t'.join(headers), comments='')\n",
    "        \n",
    "    ari = adjusted_rand_score(targets, predictions)\n",
    "    print(f'ari: {ari}')\n",
    "    return ari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df2507",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    node_features_fn = f'/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/learning_curve_analysis/input/node_features030_2_time_cross_90{i}.txt'\n",
    "    graph_targets_fn = f'/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/learning_curve_analysis/input/graph_targets030_time_cross_90{i}.txt'\n",
    "    edges_fn = '/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/gtex/input/edges.txt'\n",
    "    model_fn = '/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/gtex/GNN/trained_pytorch_model_rewired10_fold_full_dataset.pt'\n",
    "    output_fn = f'/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/learning_curve_analysis/output/predictions030_time_cross_90{i}.tsv'\n",
    "    sample_id_fn = f'/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/learning_curve_analysis/output/tcdd_sample_id030_time_cross_90{i}.txt'\n",
    "    project_id_fn = f'/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/learning_curve_analysis/output/tcdd_project_id030_time_cross_90{i}.txt'\n",
    "    gender_fn = f'/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/learning_curve_analysis/output/tcdd_gender030_time_cross_90{i}.txt'\n",
    "    dose_fn = f'/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/learning_curve_analysis/output/tcdd_dose030_time_cross_90{i}.txt'\n",
    "    # test graph_targets.txt, node_features.txt and edges.txt\n",
    "    features_exist = op.exists(node_features_fn)\n",
    "    targets_exist = op.exists(graph_targets_fn)\n",
    "    edges_exist = op.exists(edges_fn)\n",
    "    model_exists = op.exists(model_fn)\n",
    "\n",
    "    def change_key(self, old, new):\n",
    "        for _ in range(len(self)):\n",
    "            k, v = self.popitem(False)\n",
    "            self[new if old == k else k] = v\n",
    "\n",
    "    (edge_v1, edge_v2) = read_reactome_graph(edges_fn)\n",
    "    model = GNN(hidden_channels=HIDDEN_CHANNELS)\n",
    "    device = cpu = torch.device('cpu')\n",
    "\n",
    "    sd = torch.load(model_fn, map_location=device)\n",
    "    change_key(sd, 'conv1.lin_l.weight', 'conv1.lin_rel.weight')\n",
    "    change_key(sd, 'conv1.lin_l.bias', 'conv1.lin_rel.bias')\n",
    "    change_key(sd, 'conv1.lin_r.weight', 'conv1.lin_root.weight')\n",
    "    change_key(sd, 'conv2.lin_l.weight', 'conv2.lin_rel.weight')\n",
    "    change_key(sd, 'conv2.lin_l.bias', 'conv2.lin_rel.bias')\n",
    "    change_key(sd, 'conv2.lin_r.weight', 'conv2.lin_root.weight')\n",
    "    change_key(sd, 'conv3.lin_l.weight', 'conv3.lin_rel.weight')\n",
    "    change_key(sd, 'conv3.lin_l.bias', 'conv3.lin_rel.bias')\n",
    "    change_key(sd, 'conv3.lin_r.weight', 'conv3.lin_root.weight')\n",
    "    change_key(sd, 'lin.weight', 'lin.weight')\n",
    "    change_key(sd, 'lin.bias', 'lin.bias')\n",
    "\n",
    "    model.load_state_dict(sd)\n",
    "    model.eval()\n",
    "    \n",
    "    # replace final layer with new shape matching new dataset\n",
    "    model.lin = Linear(HIDDEN_CHANNELS, NEW_CHANNELS)\n",
    "\n",
    "    model.conv1.lin_rel.weight.requires_grad = False\n",
    "    model.conv1.lin_rel.bias.requires_grad = False\n",
    "    model.conv1.lin_root.weight.requires_grad = False\n",
    "    model.conv2.lin_rel.weight.requires_grad = False\n",
    "    model.conv2.lin_rel.bias.requires_grad = False\n",
    "    model.conv2.lin_root.weight.requires_grad = False\n",
    "    model.conv3.lin_rel.weight.requires_grad = False\n",
    "    model.conv3.lin_rel.bias.requires_grad = False\n",
    "    model.conv3.lin_root.weight.requires_grad = False\n",
    "    model.lin.weight.requires_grad = True\n",
    "    model.lin.bias.requires_grad = True\n",
    "\n",
    "    # for name, param in model.named_parameters(): print(name, param)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    data_list = build_reactome_graph_datalist(edge_v1, edge_v2, node_features_fn, graph_targets_fn, project_id_fn, sample_id_fn, gender_fn, dose_fn)\n",
    "    # retrain model for fine tuning transfer learning\n",
    "    train_data_list = data_list  # all data\n",
    "    print(len(train_data_list))\n",
    "    print(f'Number of training graphs: {len(train_data_list)}')\n",
    "    train_data_loader = build_reactome_graph_loader(train_data_list, BATCH_SIZE)\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(train_data_loader, device)\n",
    "        train_acc = train(train_data_loader, device)\n",
    "        print(f'Epoch: {epoch}, Train Acc: {train_acc}')\n",
    "        if train_acc == 1.0:\n",
    "            break\n",
    "\n",
    "    final_ari = test(train_data_loader, device)\n",
    "    print(f'test_ari: {final_ari}')\n",
    "\n",
    "    model_save_name = f'tuned_pytorch_tcdd_model030_time_cross_90{i}.pt'\n",
    "    path = f'/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/learning_curve_analysis/output/{model_save_name}'\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f'model saved as {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf2c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import time\n",
    "import torch\n",
    "import numpy\n",
    "import random\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from sklearn import preprocessing\n",
    "from collections import defaultdict\n",
    "from IPython.display import Javascript\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from captum.attr import Saliency, IntegratedGradients\n",
    "from torch_geometric.nn import GraphConv, global_mean_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb101d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic numbers\n",
    "INPUT_CHANNELS = 1\n",
    "OUTPUT_CHANNELS = 2\n",
    "HIDDEN_CHANNELS = 64\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.conv1 = GraphConv(INPUT_CHANNELS, hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, OUTPUT_CHANNELS)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_weight=None):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def read_reactome_graph(e_fn):\n",
    "    e_v1 = []\n",
    "    e_v2 = []\n",
    "\n",
    "    for line in open(e_fn, 'r'):\n",
    "        dt = line.split()\n",
    "        node1 = int(dt[0]) - 1  # subtracting to convert R idx to python idx\n",
    "        node2 = int(dt[1]) - 1  # \" \"\n",
    "        e_v1.append(node1)\n",
    "        e_v2.append(node2)\n",
    "\n",
    "    return e_v1, e_v2\n",
    "\n",
    "\n",
    "def build_reactome_graph_datalist(e_v1, e_v2, n_features_fn, g_targets_fn):\n",
    "    edge_index = torch.tensor([e_v1, e_v2], dtype=torch.long)\n",
    "    feature_v = numpy.loadtxt(n_features_fn)\n",
    "    t_v = numpy.loadtxt(g_targets_fn, dtype=float, delimiter=\",\")\n",
    "\n",
    "#     t_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "#     t_v = t_encoder.fit_transform(t_v)\n",
    "\n",
    "    binary_labels = (t_v > 0).astype(int)\n",
    "\n",
    "    d_list = []\n",
    "    for row_idx in range(len(feature_v)):\n",
    "        x = torch.tensor(feature_v[row_idx, :], dtype=torch.float)\n",
    "        x = x.unsqueeze(1)\n",
    "#         y = torch.tensor([t_v[row_idx]])\n",
    "        y = torch.tensor([binary_labels[row_idx]], dtype=torch.long)\n",
    "        d_list.append(Data(x=x, y=y, edge_index=edge_index))\n",
    "\n",
    "    return d_list\n",
    "\n",
    "\n",
    "def explain(m, dt, target):\n",
    "    input_mask = torch.ones(dt.edge_index.shape[1]).requires_grad_(True).to(device)\n",
    "    if m == 'ig':\n",
    "        ig = IntegratedGradients(model_forward)\n",
    "        mask = ig.attribute(input_mask, target=target,\n",
    "                            additional_forward_args=(dt,),\n",
    "                            internal_batch_size=dt.edge_index.shape[1])\n",
    "    else:\n",
    "        raise Exception('Unknown explanation method')\n",
    "\n",
    "    e_mask = np.abs(mask.cpu().detach().numpy())\n",
    "    if e_mask.max() > 0:  # avoid division by zero\n",
    "        e_mask = e_mask / e_mask.max()\n",
    "    return e_mask\n",
    "\n",
    "\n",
    "def aggregate_edge_directions(e_mask, dt):\n",
    "    edge_mask_dict = defaultdict(float)\n",
    "    for val, u, v in list(zip(e_mask, *dt.edge_index)):\n",
    "        u, v = u.item(), v.item()\n",
    "        if u > v:\n",
    "            u, v = v, u\n",
    "        edge_mask_dict[(u, v)] += val\n",
    "    return edge_mask_dict\n",
    "\n",
    "\n",
    "def model_forward(e_mask, dt):\n",
    "    batch = torch.zeros(dt.x.shape[0], dtype=int).to(device)\n",
    "    out = model(dt.x,\n",
    "                dt.edge_index,\n",
    "                batch,\n",
    "                e_mask)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcec36d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    node_features_fn = f'/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/learning_curve_analysis/input/node_features030_2_time_cross_90\",{i},\".txt'\n",
    "    graph_targets_fn = f'/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/learning_curve_analysis/input/graph_targets030_time_cross_90\",{i},\".txt'\n",
    "    edges_fn = f'/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/gtex/input/edges.txt'\n",
    "    model_fn = f'/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/learning_curve_analysis/output/tuned_pytorch_tcdd_model030_time_cross_90\",{i},\".pt'\n",
    "    output_fn = f'/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/learning_curve_analysis/output/predictions030_time_cross_90\",{i},\".tsv'\n",
    "    transformed_targets_fn = f'/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/learning_curve_analysis/output/transformed_targets030_time_cross_90\",{i},\".txt'\n",
    "    inverted_targets_fn = f'/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/learning_curve_analysis/output/inverted_targets030_time_cross_90\",{i},\".txt'\n",
    "    \n",
    "    (edge_v1, edge_v2) = read_reactome_graph(edges_fn)\n",
    "\n",
    "    data_list = build_reactome_graph_datalist(edge_v1, edge_v2, node_features_fn, graph_targets_fn)\n",
    "    data_loader = DataLoader(data_list)\n",
    "\n",
    "    # rebuild label encoder to invert numerical transformation\n",
    "    target_v = numpy.loadtxt(graph_targets_fn, dtype=str, delimiter=\",\")\n",
    "    target_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "\n",
    "    target_v = target_encoder.fit_transform(target_v)\n",
    "    path = transformed_targets_fn\n",
    "    numpy.savetxt(path, target_v, delimiter=\",\", fmt=\"%.0f\")\n",
    "    print(F\"target_v saved as {path}\")\n",
    "\n",
    "    target_l = target_encoder.inverse_transform(target_v)\n",
    "    path = inverted_targets_fn\n",
    "    numpy.savetxt(path, target_l, delimiter=\",\", fmt=\"%s\")\n",
    "    print(F\"target_l saved as {path}\")\n",
    "\n",
    "    model = GNN(hidden_channels=HIDDEN_CHANNELS)\n",
    "    device = cpu = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    path = model_fn\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    d = data_loader.dataset[0]\n",
    "    d.edge_index.shape[1]\n",
    "\n",
    "    data = data_loader.dataset[0]\n",
    "\n",
    "    for target_tissue in range(2):\n",
    "        title = 'Integrated Gradients'\n",
    "        method = 'ig'\n",
    "        data.to(device)\n",
    "        print(F\"processing tissue {target_tissue} with {title}, a.k.a. {method}\")\n",
    "        edge_mask = explain(method, data, target=target_tissue)\n",
    "        # edge_mask_dict = aggregate_edge_directions(edge_mask, data)\n",
    "        path = F\"/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/learning_curve_analysis/output/{method}_{target_tissue}030_time_cross_90\",{i},\".txt\"\n",
    "        numpy.savetxt(path, edge_mask, delimiter=\",\")\n",
    "        print(F\"{method} {target_tissue} edges saved as {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a000a767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737aaca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
