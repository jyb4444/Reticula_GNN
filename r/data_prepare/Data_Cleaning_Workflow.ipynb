{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203e4110",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92287732",
   "metadata": {},
   "source": [
    "## Load python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4624ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d17f85",
   "metadata": {},
   "source": [
    "## Load R packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7b6b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(SummarizedExperiment)\n",
    "library(recount3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6666d04",
   "metadata": {},
   "source": [
    "# Test API from Recount3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296a74aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "tryCatch({\n",
    "    rse_gene <- recount3::create_rse_manual(\n",
    "        project = \"SRP104670\",\n",
    "        project_home = \"data_sources/sra\",\n",
    "        organism = \"mouse\",\n",
    "        annotation = \"gencode_v23\",\n",
    "        type = \"gene\")\n",
    "    \n",
    "    colData(rse_gene)$sra.study_title\n",
    "    \n",
    "    },error=function(cond){\n",
    "        print(cond)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc7a655",
   "metadata": {},
   "source": [
    "# Download Data From Recount3\n",
    "\n",
    "This R code processes a manually curated list of RNA-seq projects from recount3, specified in a CSV file. It iterates through each project, retrieving metadata for those with 600 or fewer samples using create_rse_manual(), and saves the extracted metadata as a text file. Errors are handled gracefully by printing the sample count of failed projects instead of interrupting execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2772ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "data <- read.csv(\"/mnt/home/yuankeji/RanceLab/reticula_new/recount3_selection_2024-04-12.csv\")\n",
    "\n",
    "for (i in 1:nrow(data)) {\n",
    "\n",
    "  if (data$n_samples[i] <= 600) {\n",
    "      tryCatch({\n",
    "        rse_gene <- recount3::create_rse_manual(\n",
    "          project = data$project[i],\n",
    "          project_home = data$project_home[i],\n",
    "          organism = data$organism[i],\n",
    "          annotation = \"gencode_v23\",\n",
    "          type = \"gene\"\n",
    "        )\n",
    "\n",
    "        output_path <- paste0(\"/mnt/home/yuankeji/RanceLab/reticula_new/downloadFromRecount3/rse_gene_\", data$project[i], \".txt\")\n",
    "        write.table(data.frame(colData(rse_gene)), file = output_path, sep = '\\t', quote = FALSE)\n",
    "      },\n",
    "      error = function(cond){\n",
    "          print(data$n_samples[i])\n",
    "      }\n",
    "    )\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f75a7ea",
   "metadata": {},
   "source": [
    "# Extract Useful Data (sra.experiment_attributes) From Recount3 Into .csv File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f63a108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_attributes(attr_str):\n",
    "    \"\"\" Parse the attribute string into a dictionary \"\"\"\n",
    "    if pd.isna(attr_str):\n",
    "        return {}\n",
    "    attrs = attr_str.split('|')\n",
    "    attr_dict = {}\n",
    "    for attr in attrs:\n",
    "        if ';;' in attr:\n",
    "            key, value = attr.split(';;', 1)\n",
    "            attr_dict[key] = value\n",
    "    return attr_dict\n",
    "\n",
    "# Directory containing the files\n",
    "input_directory = '/mnt/home/yuankeji/RanceLab/reticula_new/downloadFromRecount3/'\n",
    "output_directory = '/mnt/home/yuankeji/RanceLab/reticula_new/'\n",
    "output_file = 'combined_data_all_duplicate.csv'\n",
    "\n",
    "# List all text files in the directory\n",
    "file_paths = glob.glob(os.path.join(input_directory, '*.txt'))\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "combined_df = pd.DataFrame()\n",
    "# Process each file\n",
    "for index, file_path in enumerate(file_paths):\n",
    "    try:\n",
    "        print(f\"Processing file {index}: {file_path}\")\n",
    "        # Read the file\n",
    "        data = pd.read_csv(file_path, sep='\\t', index_col=0)\n",
    "\n",
    "        # Drop duplicate entries in 'sra.experiment_attributes' before parsing\n",
    "        # unique_attributes = data['sra.sample_attributes'].drop_duplicates()\n",
    "        unique_attributes = data['sra.sample_attributes']\n",
    "\n",
    "        # Apply the parsing function to the unique 'sra.experiment_attributes' column\n",
    "        unique_attributes = unique_attributes.apply(parse_attributes)\n",
    "\n",
    "        # Convert the dictionary column to separate columns\n",
    "        attributes_df = pd.DataFrame(unique_attributes.tolist(), index=unique_attributes.index)\n",
    "\n",
    "        # Set the index name to the file name for clarity\n",
    "        attributes_df.index.name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "        # Combine with the main DataFrame\n",
    "        combined_df = pd.concat([combined_df, attributes_df])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        break\n",
    "\n",
    "combined_df.to_csv(os.path.join(output_directory, output_file), sep='\\t')\n",
    "\n",
    "\n",
    "print(f\"Data combined and saved to {os.path.join(output_directory, output_file)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d441b3",
   "metadata": {},
   "source": [
    "# If the combined_data_all_duplicate.csv file is too large, you can use following code to split it into several part so that can run the following code without interrupting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d87285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split file at the begining\n",
    "df = pd.read_csv('/Users/kejiyuan/Desktop/RanceLab/reticula_new/original_data/combined_data_all_duplicate.csv', sep='\\t', dtype=str)\n",
    "\n",
    "part_size = len(df) // 3\n",
    "\n",
    "df.iloc[:part_size].to_csv('/Users/kejiyuan/Desktop/RanceLab/reticula_new/original_data/part1.csv', sep='\\t', index=False)\n",
    "df.iloc[part_size:2*part_size].to_csv('/Users/kejiyuan/Desktop/RanceLab/reticula_new/original_data/part2.csv', sep='\\t', index=False)\n",
    "df.iloc[2*part_size:].to_csv('/Users/kejiyuan/Desktop/RanceLab/reticula_new/original_data/part3.csv', sep='\\t', index=False)\n",
    "\n",
    "# \n",
    "# combine files together after removing and filtering all useless data\n",
    "df_part1 = pd.read_csv('/Users/kejiyuan/Desktop/RanceLab/reticula_new/removingFile_part1.6.csv', sep='\\t', dtype=str)\n",
    "df_part2 = pd.read_csv('/Users/kejiyuan/Desktop/RanceLab/reticula_new/removingFile_part2.6.csv', sep='\\t', dtype=str)\n",
    "df_part3 = pd.read_csv('/Users/kejiyuan/Desktop/RanceLab/reticula_new/removingFile_part3.6.csv', sep='\\t', dtype=str)\n",
    "\n",
    "df_combined = pd.concat([df_part1, df_part2, df_part3], ignore_index=True)\n",
    "\n",
    "df_combined.to_csv('/Users/kejiyuan/Desktop/RanceLab/reticula_new/original_data/combined_data_all_duplicate_restored.csv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ee967b",
   "metadata": {},
   "source": [
    "# Filter Data Based On Comparison File (You need to manually generate the comparison file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1707e6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = '/Users/kejiyuan/Desktop/RanceLab/reticula_new/renamed_data3.csv'\n",
    "output_file_path = '/Users/kejiyuan/Desktop/RanceLab/reticula_new/renamed_data4.csv'\n",
    "df = pd.read_csv('/Users/kejiyuan/Desktop/RanceLab/reticula_new/original_data/delete_these_smaller1.txt', sep='\\t', encoding='utf-8')\n",
    "# df = pd.read_excel('/Users/kejiyuan/Desktop/RanceLab/reticula_new/original_data/DeleteTheseTerms.xlsx')\n",
    "df_input = pd.read_csv(input_file_path, sep=',', dtype=str)\n",
    "\n",
    "def remove_rows_and_columns(df, column_name, keywords):\n",
    "    if column_name not in df.columns:\n",
    "        print(f\"Column '{column_name}' does not exist in the DataFrame.\")\n",
    "        return df\n",
    "    \n",
    "    if keywords[0] == '*':\n",
    "        # Delete rows where the column has non-empty values\n",
    "        condition = df[column_name].notna()\n",
    "        indices_to_delete = df[condition].index\n",
    "        df.drop(indices_to_delete, inplace=True)\n",
    "    else:\n",
    "        # Escape special characters in keywords\n",
    "        escaped_keywords = [re.escape(keyword) for keyword in keywords]\n",
    "        # Generate regular expression pattern\n",
    "        pattern = '|'.join(escaped_keywords)\n",
    "        # indices_to_delete = df[df[column_name].isin(keywords)].index\n",
    "        condition = df[column_name].str.contains(pattern, case=False, na=False, regex=True)\n",
    "\n",
    "        indices_to_delete = df[condition].index\n",
    "\n",
    "        df.drop(indices_to_delete, inplace=True)\n",
    "\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    df.dropna(axis=0, how='all', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "columns_as_row = pd.DataFrame([df.columns.tolist()], columns=df.columns)\n",
    "# df = pd.concat([columns_as_row, df], ignore_index=True)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    rest_of_values = row.iloc[1:].dropna().values  \n",
    "\n",
    "    if len(rest_of_values) > 0:  \n",
    "        first_column_value = row.iloc[0] \n",
    "        print(index, df_input.shape, first_column_value, rest_of_values)\n",
    "        df_input = remove_rows_and_columns(df_input, first_column_value, rest_of_values)\n",
    "\n",
    "\n",
    "df_input.to_csv(output_file_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaab705",
   "metadata": {},
   "source": [
    "# Export a file based on key-value pairs of column names to facilitate manual deletion of useless data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b26c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that takes a sequence and moves nulls to the end\n",
    "def move_nulls_to_end(series):\n",
    "    non_nulls = series.dropna()\n",
    "    nulls = series[series.isnull()]\n",
    "    return pd.concat([non_nulls, nulls]).reset_index(drop=True)\n",
    "\n",
    "def process_csv(input_file_path, output_file_path):\n",
    "    df = pd.read_csv(input_file_path, sep='\\t', dtype=str)\n",
    "    del df[df.columns[0]]\n",
    "\n",
    "    unique_data = {}\n",
    "    \n",
    "    # Iterate through each column and remove duplicate values\n",
    "    for column in df.columns:\n",
    "        unique_data[column] = df[column].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    unique_df = pd.DataFrame.from_dict(unique_data)\n",
    "\n",
    "    unique_df = unique_df.transpose()\n",
    "\n",
    "    sorted_df = unique_df.apply(move_nulls_to_end, axis=1)\n",
    "\n",
    "    sorted_df.to_csv(output_file_path, sep='\\t', index=True)\n",
    "\n",
    "# Call the function, which needs to be replaced with the actual file path\n",
    "input_file_path = '/Users/kejiyuan/Desktop/RanceLab/reticula_new/renamed_data4.csv'\n",
    "output_file_path = '/Users/kejiyuan/Desktop/RanceLab/reticula_new/key_val_pair.txt'\n",
    "process_csv(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc1243f",
   "metadata": {},
   "source": [
    "# Produce Project ID File in Bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b945c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory=\"/Users/kejiyuan/Desktop/RanceLab/reticula_new/original_data/downloadFromRecount3/\"\n",
    "\n",
    "for file in \"$input_directory\"/*.txt; do\n",
    "    # Cut the first column and print each row with the filename\n",
    "    awk -v fname=\"$file\" '{print fname \"\\t\" $1}' \"$file\"\n",
    "done > \"/Users/kejiyuan/Desktop/RanceLab/reticula_new/SRR.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e83a02",
   "metadata": {},
   "source": [
    "# Merge Project ID Into Final File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830790c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files\n",
    "srr_df = pd.read_csv('/Users/kejiyuan/Desktop/RanceLab/reticula_new/SRR.csv', header=None, names=['path', 'SRR'], sep='\\t', quotechar='\"')\n",
    "recount3_selection_df = pd.read_csv('/Users/kejiyuan/Desktop/RanceLab/reticula_new/original_data/recount3_selection_2024-04-12.csv', sep=',', quotechar='\"')\n",
    "renamed_data4_df = pd.read_csv('/Users/kejiyuan/Desktop/RanceLab/reticula_new/renamed_data4.csv', sep='\\t', quotechar='\"')\n",
    "\n",
    "# Extract the project ID from the file path in SRR.csv\n",
    "\n",
    "srr_df['project_id'] = srr_df['path'].str.replace(r'/Users/kejiyuan/Desktop/RanceLab/reticula_new/original_data/downloadFromRecount3//rse_gene_', '').str.replace(r'.txt', '')\n",
    "\n",
    "# Remove rows where SRR is 'rail_id'\n",
    "srr_df = srr_df[srr_df['SRR'] != 'rail_id']\n",
    "\n",
    "# Merge SRR.csv with recount3_selection.csv on the extracted project ID\n",
    "merged_df = pd.merge(srr_df, recount3_selection_df, left_on='project_id', right_on='project', how='left')\n",
    "\n",
    "# Merge the result with renamed_data4.csv on the SRR identifier\n",
    "final_df = pd.merge(merged_df, renamed_data4_df, on='SRR', how='left')\n",
    "\n",
    "# Select columns to keep from recount3_selection_2024-04-12.csv excluding 'project'\n",
    "columns_to_keep = ['organism', 'project_home', 'n_samples', 'study_title', 'study_abstract'] + list(renamed_data4_df.columns)\n",
    "final_df = final_df[columns_to_keep]\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "final_df.to_csv('/Users/kejiyuan/Desktop/RanceLab/reticula_new/renamed_data5.csv', index=False)\n",
    "\n",
    "print(\"Merging completed. Output saved as 'renamed_data5.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5957fbe0",
   "metadata": {},
   "source": [
    "# Merge Columns Name Based on Specific File (If it is necessary.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa275605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_df = pd.read_csv('/Users/kejiyuan/Desktop/RanceLab/reticula_new/original_data/combined_data_all_duplicate_restored.csv', sep='\\t')\n",
    "\n",
    "# Load the mapping file\n",
    "mapping_df = pd.read_csv('/Users/kejiyuan/Desktop/RanceLab/reticula_new/original_data/ColumnRenaming.txt', sep='\\t', dtype=str)\n",
    "mapping_df.loc[mapping_df['Old Column Name'] == 'Unnamed: 0', 'New Column'] = 'SRR'  # Make sure this mapping is correct\n",
    "\n",
    "# Apply renaming\n",
    "mapping_dict = dict(zip(mapping_df['Old Column Name'], mapping_df['New Column']))\n",
    "data_df.rename(columns=mapping_dict, inplace=True)\n",
    "\n",
    "# Make sure 'SRR' is the first column\n",
    "# We assume 'SRR' is already in the DataFrame from the renaming step\n",
    "columns = ['SRR'] + [col for col in data_df.columns if col != 'SRR']\n",
    "data_df = data_df[columns]\n",
    "\n",
    "# Define the function for merging columns\n",
    "def same_merge(x):\n",
    "    if len(x.dropna()) == 0:\n",
    "        return pd.NA\n",
    "    return ','.join(x.dropna().astype(str))\n",
    "\n",
    "# Apply merging and save\n",
    "grouped = data_df.groupby(by=data_df.columns, axis=1)\n",
    "data_df = grouped.apply(lambda x: x.apply(same_merge, axis=1))\n",
    "\n",
    "# Confirm 'SRR' is the first column and print final columns\n",
    "print(\"Final DataFrame columns:\", data_df.columns)\n",
    "\n",
    "# Save the results\n",
    "data_df.to_csv('/Users/kejiyuan/Desktop/RanceLab/reticula_new/renamed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4e817c",
   "metadata": {},
   "source": [
    "# Final Check On Data\n",
    "# You can delete by number of tissue and specific tissue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851a33a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/kejiyuan/Desktop/RanceLab/reticula_new/rse_gene/CleanedSamplesGNN_v1.txt', sep='\\t')\n",
    "\n",
    "# Count the number of each issue\n",
    "tissue_counts = data['Major_tissue'].value_counts()\n",
    "\n",
    "# Delete all rows with number of tissue less than or equal to 20\n",
    "tissues_to_remove = tissue_counts[tissue_counts <= 20].index\n",
    "data = data[~data['Major_tissue'].isin(tissues_to_remove)]\n",
    "\n",
    "# Delete all rows of spleen\n",
    "data = data[data['Major_tissue'] != 'Spleen']\n",
    "\n",
    "data.to_csv('/Users/kejiyuan/Desktop/RanceLab/reticula_new/rse_gene/CleanedSamplesGNN_v2.txt', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1c127e",
   "metadata": {},
   "source": [
    "# Produce rse_gene.Rdata File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb66a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "recount_data <- read.csv(\"/Users/kejiyuan/Desktop/RanceLab/reticula_new/recount3_selection_2024-04-12.csv\")\n",
    "cleaned_data <- read.csv('/Users/kejiyuan/Desktop/RanceLab/reticula_new/rse_gene/CleanedSamplesGNN_v3.csv', sep='.')\n",
    "\n",
    "final_result <- NULL\n",
    "\n",
    "print(nrow(recount_data))\n",
    "\n",
    "for (i in 1:nrow(recount_data)) {\n",
    "      if (recount_data[i, 'study_title'] %in% cleaned_data$Title) {\n",
    "      print(paste(\"Current running\", i))\n",
    "      if (recount_data$n_samples[i] <= 600) {\n",
    "          tryCatch({\n",
    "            rse_gene <- recount3::create_rse_manual(\n",
    "              project = recount_data$project[i],\n",
    "              project_home = recount_data$project_home[i],\n",
    "              organism = recount_data$organism[i],\n",
    "              annotation = \"gencode_v23\",\n",
    "              type = \"gene\"\n",
    "            )\n",
    "\n",
    "            sample_list <- cleaned_data[,'SRR']\n",
    "            keep <- intersect(sample_list, colnames(rse_gene))\n",
    "            temp_rse_gene <- rse_gene[,keep]\n",
    "              \n",
    "            gene_order <- colData(temp_rse_gene)$external_id\n",
    "            cur_obj <- merge(colData(temp_rse_gene), cleaned_data, by.x=\"external_id\", by.y = \"SRR\", all.x=TRUE)\n",
    "            ordered_indices <- match(gene_order, cur_obj$external_id)\n",
    "            cur_obj <- cur_obj[ordered_indices, ]\n",
    "              \n",
    "            colData(temp_rse_gene) <- cur_obj\n",
    "            if (is.null(final_result)) {\n",
    "                final_result <- temp_rse_gene\n",
    "            } else {\n",
    "                final_result <- cbind(final_result, temp_rse_gene)\n",
    "            }\n",
    "          },\n",
    "          error = function(cond){\n",
    "              print(\"-----------------\")\n",
    "              print(recount_data$n_samples[i])\n",
    "              print(recount_data$project[i])\n",
    "              print(i)\n",
    "          }\n",
    "        )\n",
    "      }\n",
    "    }\n",
    "  \n",
    "}\n",
    "\n",
    "save(final_result, file = \"/Users/kejiyuan/Desktop/RanceLab/reticula_new/rse_gene/rse_gene.Rdata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ed6ecc",
   "metadata": {},
   "source": [
    "# Produce GEO_model_validation_rse_gene.Rdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfcdb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "recount_data <- read.csv(\"/Users/kejiyuan/Desktop/RanceLab/reticula_new/recount3_selection_2024-04-12.csv\")\n",
    "cleaned_data <- read.csv('/Users/kejiyuan/Desktop/RanceLab/reticula_new/rse_gene/GEO_model_validation.txt', sep='\\t')\n",
    "\n",
    "final_result <- NULL\n",
    "\n",
    "print(nrow(recount_data))\n",
    "needed_project_ids <- c('SRP131784', 'SRP090688', 'SRP161461', 'SRP075814', 'SRP049440')\n",
    "\n",
    "for (i in 1:nrow(recount_data)) {\n",
    "    print(paste(\"Current running\", i))\n",
    "    # Check if the current project ID is one of the needed ones\n",
    "    if (!(recount_data$project[i] %in% needed_project_ids)) {\n",
    "        next  # Skip the rest of this iteration if the project ID is not needed\n",
    "    }\n",
    "\n",
    "    tryCatch({\n",
    "        rse_gene <- recount3::create_rse_manual(\n",
    "          project = recount_data$project[i],\n",
    "          project_home = recount_data$project_home[i],\n",
    "          organism = recount_data$organism[i],\n",
    "          annotation = \"gencode_v23\",\n",
    "          type = \"gene\"\n",
    "        )\n",
    "\n",
    "        sample_list <- cleaned_data[,'SRR']\n",
    "        keep <- intersect(sample_list, colnames(rse_gene))\n",
    "        temp_rse_gene <- rse_gene[,keep]\n",
    "#         rownames(cleaned_data) <- cleaned_data$SRR\n",
    "        \n",
    "        gene_order <- colData(temp_rse_gene)$external_id\n",
    "        cur_obj <- merge(colData(temp_rse_gene), cleaned_data, by.x=\"external_id\", by.y = \"SRR\", all.x=TRUE)\n",
    "        ordered_indices <- match(gene_order, cur_obj$external_id)\n",
    "        cur_obj <- cur_obj[ordered_indices, ]\n",
    "        \n",
    "        colData(temp_rse_gene) <- cur_obj\n",
    "        if (is.null(final_result)) {\n",
    "            final_result <- temp_rse_gene\n",
    "        } else {\n",
    "            final_result <- cbind(final_result, temp_rse_gene)\n",
    "        }\n",
    "    },\n",
    "    error = function(cond){\n",
    "        print(\"-----------------\")\n",
    "        print(recount_data$n_samples[i])\n",
    "        print(recount_data$project[i])\n",
    "        print(i)\n",
    "    })\n",
    "\n",
    "}\n",
    "\n",
    "save(final_result, file = \"/Users/kejiyuan/Desktop/RanceLab/reticula_new/rse_gene/GEO_model_validation_rse_gene.Rdata\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72b64ab",
   "metadata": {},
   "source": [
    "# Produce ReactionNetwork_Rel.txt\n",
    "\n",
    "1. Install Docker \n",
    "2. Find reactome/graph on dockerhub and running:\n",
    "    docker run -p 7474:7474 -p 7687:7687 -e NEO4J_dbms_memory_heap_maxSize=8g reactome/graphdb:latest\n",
    "3. The username is \"Neo4j\" and the password is \"admin\".\n",
    "4. Using the following two queries to get the data:\n",
    "    \"MATCH (r2:ReactionLikeEvent {speciesName:\"Mus musculus\"})-[:precedingEvent]->(r1:ReactionLikeEvent {speciesName:\"Mus musculus\"}) RETURN r1.stId as `Preceding Reaction`, r2.stId as `Following Reaction`\"\n",
    "    \n",
    "    \"MATCH (r1:ReactionLikeEvent {speciesName:\"Mus musculus\"})-[:output]->(PhysicalEntity)<-[:physicalEntity]-(CatalystActivity)<-[:catalystActivity]-(r2:ReactionLikeEvent {speciesName:\"Mus musculus\"}) RETURN r1.stId as `Preceding Reaction`,r2.stId as `Following Reaction`\"\n",
    "   \n",
    "5. Download teh result as Mouse1.csv and Mouse2.csv file.\n",
    "6. Using following code to integrate data together:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318e0dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = pd.read_csv(os.getcwd() + \"/Mouse1.csv\")\n",
    "file2 = pd.read_csv(os.getcwd() + \"/Mouse2.csv\")\n",
    "\n",
    "def insertCol(file):\n",
    "    file['Relationship'] = \"Preceding\"\n",
    "    col_names = file.columns.tolist()\n",
    "    new_col_order = [col_names[0], \"Relationship\"] + col_names[1:-1]\n",
    "    file = file[new_col_order]\n",
    "    return file\n",
    "\n",
    "file1 = insertCol(file1)\n",
    "file2 = insertCol(file2)\n",
    "\n",
    "merged_df = pd.concat([file1, file2], ignore_index=True)\n",
    "\n",
    "merged_df.to_csv(\"ReactionNetwork_Rel.txt\", sep=\"\\t\", index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82dc73e",
   "metadata": {},
   "source": [
    "# Produce ReactionToPathway_Rel.csv file\n",
    "\n",
    "1. Install Docker \n",
    "2. Find reactome/graph on dockerhub and running:\n",
    "    docker run -p 7474:7474 -p 7687:7687 -e NEO4J_dbms_memory_heap_maxSize=8g reactome/graphdb:latest\n",
    "3. The username is \"Neo4j\" and the password is \"admin\".\n",
    "4. Using following query to get the relationship between pathway and reaction\n",
    "    \"MATCH (p:Pathway {speciesName:\"Mus musculus\"})-[:hasEvent]->(r:ReactionLikeEvent)\n",
    "    RETURN p.stId as Pathway, r.stId as ReactionLikeEvent, r.displayName as Title\"\n",
    "5. Download teh result as ReactionToPathway_Rel.csv file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
