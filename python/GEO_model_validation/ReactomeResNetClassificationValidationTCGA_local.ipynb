{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "452a73e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import random\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import sklearn\n",
    "import torch.nn.functional as nn_func\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "\n",
    "random.seed = 88888888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "768ff2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features exist: True, targets exist: True, model exists: True\n"
     ]
    }
   ],
   "source": [
    "node_features_fn = '/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/GEO_model_validation/input/node_features.txt'\n",
    "graph_targets_fn = '/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/GEO_model_validation/input/graph_targets.txt'\n",
    "model_fn = '/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/GEO_model_training/GNN/trained_pytorch_model_fold_full_dataset.pt'\n",
    "output_fn = '/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/GEO_model_validation/output/resnet_predictions.tsv'\n",
    "sampleID_fn = '/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/GEO_model_validation/input/sample_id.txt'\n",
    "\n",
    "features_exist = op.exists(node_features_fn)\n",
    "targets_exist = op.exists(graph_targets_fn)\n",
    "model_exists = op.exists(model_fn)\n",
    "\n",
    "print(f'features exist: {features_exist},'\n",
    "      f' targets exist: {targets_exist},'\n",
    "      f' model exists: {model_exists}')\n",
    "assert features_exist\n",
    "assert targets_exist\n",
    "assert model_exists\n",
    "\n",
    "# magic numbers\n",
    "INPUT_CHANNELS = 1\n",
    "OUTPUT_CHANNELS = 51\n",
    "NEW_CHANNELS = 13\n",
    "HIDDEN_CHANNELS = 64\n",
    "BATCH_SIZE = 64\n",
    "BENCHMARKING = False\n",
    "EPOCHS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4eed64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_reactome_graph(e_fn):\n",
    "    e_v1 = []\n",
    "    e_v2 = []\n",
    "\n",
    "    for line in open(e_fn, 'r'):\n",
    "        dt = line.split()\n",
    "        node1 = int(dt[0]) - 1  # subtracting to convert R idx to python idx\n",
    "        node2 = int(dt[1]) - 1  # \" \"\n",
    "        e_v1.append(node1)\n",
    "        e_v2.append(node2)\n",
    "\n",
    "    return e_v1, e_v2\n",
    "\n",
    "\n",
    "def build_resnet_datalist(n_features_fn, g_targets_fn, s_fn):\n",
    "    feature_v = numpy.loadtxt(n_features_fn)\n",
    "    target_v = numpy.loadtxt(g_targets_fn, dtype=str, delimiter=\",\")\n",
    "    sampleID_v = numpy.loadtxt(s_fn, dtype=str, delimiter=\",\")\n",
    "\n",
    "    target_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "    target_v = target_encoder.fit_transform(target_v)\n",
    "    label_mapping = dict(zip(target_encoder.transform(target_encoder.classes_), target_encoder.classes_))\n",
    "    print(label_mapping)\n",
    "\n",
    "    d_list = []\n",
    "    for row_idx in range(len(feature_v)):\n",
    "        x = torch.tensor(feature_v[row_idx, :], dtype=torch.float)\n",
    "        x = x.reshape(2, 8, 491)\n",
    "        y = torch.tensor([target_v[row_idx]])\n",
    "        sample_id = sampleID_v[row_idx]\n",
    "        tissue = label_mapping[target_v[row_idx]]\n",
    "        \n",
    "        d_list.append({'x': x, 'y': y, 'sid': sample_id, 'tissue': tissue})\n",
    "\n",
    "    return d_list\n",
    "\n",
    "\n",
    "def build_reactome_graph_loader(d_list, batch_size):\n",
    "    loader = DataLoader(d_list, batch_size=batch_size, shuffle=False)  # True)\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "def train(loader, dv):\n",
    "    model.train()\n",
    "\n",
    "    correct = 0\n",
    "    for batch in loader:  # Iterate in batches over the training dataset.\n",
    "        x = batch['x'].to(dv)\n",
    "        y = batch['y'].to(dv)\n",
    "        out = model(x)  # Perform a single forward pass.\n",
    "        y = torch.squeeze(y)\n",
    "        loss = criterion(out, y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == y).sum())  # Check against ground-truth labels.\n",
    "    return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "def test(loader, dv):\n",
    "    model.eval()\n",
    "\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    sample_ids = []\n",
    "    tissues = []\n",
    "    confidences = []\n",
    "    for batch in loader:  # Iterate in batches over the test dataset.\n",
    "        x = batch['x'].to(dv)\n",
    "        y = batch['y'].to(dv)\n",
    "        targets += torch.Tensor.tolist(torch.squeeze(y))\n",
    "        sample_ids += batch['sid']\n",
    "        tissues += batch['tissue']\n",
    "        out = model(x)  # Perform a single forward pass.\n",
    "        prob = torch.softmax(out, dim=1)\n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        predictions += torch.Tensor.tolist(pred)\n",
    "        confidences += torch.Tensor.tolist(prob)\n",
    "        \n",
    "    # Save targets, predictions, and confidences to a file\n",
    "    num_classes = len(confidences[0])\n",
    "    # Flatten confidences and create data for saving\n",
    "    data_to_save = []\n",
    "    for i in range(len(targets)):\n",
    "        row = [sample_ids[i], tissues[i], targets[i], predictions[i]] + confidences[i]\n",
    "        data_to_save.append(row)\n",
    "    data_to_save = numpy.array(data_to_save)\n",
    "    print(data_to_save)\n",
    "    \n",
    "    fmt = ['%s', '%s', '%s', '%s'] + ['%s' for _ in range(num_classes)]\n",
    "    \n",
    "    headers = ['sample_ids', 'tissues', 'target', 'prediction'] + [f'confidence_class_{i}' for i in range(num_classes)]\n",
    "    numpy.savetxt(output_fn, data_to_save, fmt='\\t'.join(fmt), delimiter='\\t', header='\\t'.join(headers), comments='')\n",
    "    ari = adjusted_rand_score(targets, predictions)\n",
    "    print(f'ari: {ari}')\n",
    "    return ari\n",
    "\n",
    "def change_key(self, old, new):\n",
    "    for _ in range(len(self)):\n",
    "        k, v = self.popitem(False)\n",
    "        self[new if old == k else k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3c1f83a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(2, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet18(num_classes=26)\n",
    "conv1 = model.conv1\n",
    "model.conv1 = nn.Conv2d(2,\n",
    "                        conv1.out_channels,\n",
    "                        conv1.kernel_size,\n",
    "                        conv1.stride,\n",
    "                        conv1.padding,\n",
    "                        conv1.dilation,\n",
    "                        conv1.groups,\n",
    "                        conv1.bias)\n",
    "device = cpu = torch.device('cpu')\n",
    "\n",
    "sd = torch.load(model_fn, map_location=device)\n",
    "\n",
    "model.load_state_dict(sd, strict=False)\n",
    "\n",
    "model.fc = Linear(in_features=model.fc.in_features,\n",
    "                  out_features=NEW_CHANNELS,\n",
    "                  bias=True)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28f5ee06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\"Adipose\"', 1: '\"Brain\"', 2: '\"Eye\"', 3: '\"Heart\"', 4: '\"Intestine\"', 5: '\"Kidney\"', 6: '\"Liver\"', 7: '\"Lung\"', 8: '\"MOE\"', 9: '\"Muscle\"', 10: '\"Pancreas\"', 11: '\"Skin\"', 12: '\"Testes\"'}\n",
      "1445\n",
      "723\n",
      "Number of training graphs: 723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/yuankeji/anaconda3/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Acc: 0.4854771784232365\n",
      "Epoch: 1, Train Acc: 0.669432918395574\n",
      "Epoch: 2, Train Acc: 0.7607192254495159\n",
      "Epoch: 3, Train Acc: 0.8105117565698479\n",
      "Epoch: 4, Train Acc: 0.8312586445366529\n",
      "Epoch: 5, Train Acc: 0.8464730290456431\n",
      "Epoch: 6, Train Acc: 0.8575380359612724\n",
      "Epoch: 7, Train Acc: 0.8713692946058091\n",
      "Epoch: 8, Train Acc: 0.8824343015214384\n",
      "Epoch: 9, Train Acc: 0.8921161825726142\n",
      "Epoch: 10, Train Acc: 0.8962655601659751\n",
      "Epoch: 11, Train Acc: 0.9017980636237898\n",
      "Epoch: 12, Train Acc: 0.9031811894882434\n",
      "Epoch: 13, Train Acc: 0.9073305670816044\n",
      "Epoch: 14, Train Acc: 0.9087136929460581\n",
      "Epoch: 15, Train Acc: 0.9100968188105117\n",
      "Epoch: 16, Train Acc: 0.9156293222683264\n",
      "Epoch: 17, Train Acc: 0.91701244813278\n",
      "Epoch: 18, Train Acc: 0.9197786998616874\n",
      "Epoch: 19, Train Acc: 0.9197786998616874\n",
      "Epoch: 20, Train Acc: 0.9225449515905948\n",
      "Epoch: 21, Train Acc: 0.9239280774550485\n",
      "Epoch: 22, Train Acc: 0.9266943291839558\n",
      "Epoch: 23, Train Acc: 0.9280774550484094\n",
      "Epoch: 24, Train Acc: 0.9280774550484094\n",
      "Epoch: 25, Train Acc: 0.9308437067773168\n",
      "Epoch: 26, Train Acc: 0.9322268326417704\n",
      "Epoch: 27, Train Acc: 0.9336099585062241\n",
      "Epoch: 28, Train Acc: 0.9336099585062241\n",
      "Epoch: 29, Train Acc: 0.9363762102351314\n",
      "Epoch: 30, Train Acc: 0.9377593360995851\n",
      "Epoch: 31, Train Acc: 0.9405255878284924\n",
      "Epoch: 32, Train Acc: 0.9405255878284924\n",
      "Epoch: 33, Train Acc: 0.9405255878284924\n",
      "Epoch: 34, Train Acc: 0.941908713692946\n",
      "Epoch: 35, Train Acc: 0.941908713692946\n",
      "Epoch: 36, Train Acc: 0.9446749654218534\n",
      "Epoch: 37, Train Acc: 0.9446749654218534\n",
      "Epoch: 38, Train Acc: 0.9446749654218534\n",
      "Epoch: 39, Train Acc: 0.9446749654218534\n",
      "Epoch: 40, Train Acc: 0.9446749654218534\n",
      "Epoch: 41, Train Acc: 0.946058091286307\n",
      "Epoch: 42, Train Acc: 0.9474412171507607\n",
      "Epoch: 43, Train Acc: 0.9474412171507607\n",
      "Epoch: 44, Train Acc: 0.9488243430152143\n",
      "Epoch: 45, Train Acc: 0.9488243430152143\n",
      "Epoch: 46, Train Acc: 0.9488243430152143\n",
      "Epoch: 47, Train Acc: 0.9488243430152143\n",
      "Epoch: 48, Train Acc: 0.950207468879668\n",
      "Epoch: 49, Train Acc: 0.9515905947441217\n",
      "Epoch: 50, Train Acc: 0.9529737206085753\n",
      "Epoch: 51, Train Acc: 0.9529737206085753\n",
      "Epoch: 52, Train Acc: 0.9543568464730291\n",
      "Epoch: 53, Train Acc: 0.9543568464730291\n",
      "Epoch: 54, Train Acc: 0.9543568464730291\n",
      "Epoch: 55, Train Acc: 0.9543568464730291\n",
      "Epoch: 56, Train Acc: 0.9543568464730291\n",
      "Epoch: 57, Train Acc: 0.9543568464730291\n",
      "Epoch: 58, Train Acc: 0.9543568464730291\n",
      "Epoch: 59, Train Acc: 0.9557399723374828\n",
      "Epoch: 60, Train Acc: 0.9571230982019364\n",
      "Epoch: 61, Train Acc: 0.9571230982019364\n",
      "Epoch: 62, Train Acc: 0.9571230982019364\n",
      "Epoch: 63, Train Acc: 0.9571230982019364\n",
      "Epoch: 64, Train Acc: 0.9571230982019364\n",
      "Epoch: 65, Train Acc: 0.9571230982019364\n",
      "Epoch: 66, Train Acc: 0.9571230982019364\n",
      "Epoch: 67, Train Acc: 0.9571230982019364\n",
      "Epoch: 68, Train Acc: 0.9571230982019364\n",
      "Epoch: 69, Train Acc: 0.9598893499308437\n",
      "Epoch: 70, Train Acc: 0.9598893499308437\n",
      "Epoch: 71, Train Acc: 0.9626556016597511\n",
      "Epoch: 72, Train Acc: 0.9654218533886584\n",
      "Epoch: 73, Train Acc: 0.966804979253112\n",
      "Epoch: 74, Train Acc: 0.966804979253112\n",
      "Epoch: 75, Train Acc: 0.966804979253112\n",
      "Epoch: 76, Train Acc: 0.9681881051175657\n",
      "Epoch: 77, Train Acc: 0.9681881051175657\n",
      "Epoch: 78, Train Acc: 0.9681881051175657\n",
      "Epoch: 79, Train Acc: 0.9681881051175657\n",
      "Epoch: 80, Train Acc: 0.9681881051175657\n",
      "Epoch: 81, Train Acc: 0.9695712309820194\n",
      "Epoch: 82, Train Acc: 0.9695712309820194\n",
      "Epoch: 83, Train Acc: 0.9723374827109267\n",
      "Epoch: 84, Train Acc: 0.9737206085753803\n",
      "Epoch: 85, Train Acc: 0.9737206085753803\n",
      "Epoch: 86, Train Acc: 0.9737206085753803\n",
      "Epoch: 87, Train Acc: 0.9737206085753803\n",
      "Epoch: 88, Train Acc: 0.9737206085753803\n",
      "Epoch: 89, Train Acc: 0.9737206085753803\n",
      "Epoch: 90, Train Acc: 0.9737206085753803\n",
      "Epoch: 91, Train Acc: 0.9737206085753803\n",
      "Epoch: 92, Train Acc: 0.9737206085753803\n",
      "Epoch: 93, Train Acc: 0.9737206085753803\n",
      "Epoch: 94, Train Acc: 0.9737206085753803\n",
      "Epoch: 95, Train Acc: 0.9737206085753803\n",
      "Epoch: 96, Train Acc: 0.9737206085753803\n",
      "Epoch: 97, Train Acc: 0.975103734439834\n",
      "Epoch: 98, Train Acc: 0.9764868603042877\n",
      "Epoch: 99, Train Acc: 0.9764868603042877\n",
      "Epoch: 100, Train Acc: 0.9764868603042877\n",
      "Epoch: 101, Train Acc: 0.9764868603042877\n",
      "Epoch: 102, Train Acc: 0.9764868603042877\n",
      "Epoch: 103, Train Acc: 0.9778699861687413\n",
      "Epoch: 104, Train Acc: 0.979253112033195\n",
      "Epoch: 105, Train Acc: 0.979253112033195\n",
      "Epoch: 106, Train Acc: 0.979253112033195\n",
      "Epoch: 107, Train Acc: 0.979253112033195\n",
      "Epoch: 108, Train Acc: 0.979253112033195\n",
      "Epoch: 109, Train Acc: 0.979253112033195\n",
      "Epoch: 110, Train Acc: 0.979253112033195\n",
      "Epoch: 111, Train Acc: 0.979253112033195\n",
      "Epoch: 112, Train Acc: 0.979253112033195\n",
      "Epoch: 113, Train Acc: 0.979253112033195\n",
      "Epoch: 114, Train Acc: 0.979253112033195\n",
      "Epoch: 115, Train Acc: 0.9806362378976486\n",
      "Epoch: 116, Train Acc: 0.9806362378976486\n",
      "Epoch: 117, Train Acc: 0.9806362378976486\n",
      "Epoch: 118, Train Acc: 0.9806362378976486\n",
      "Epoch: 119, Train Acc: 0.9820193637621023\n",
      "Epoch: 120, Train Acc: 0.983402489626556\n",
      "Epoch: 121, Train Acc: 0.983402489626556\n",
      "Epoch: 122, Train Acc: 0.9847856154910097\n",
      "Epoch: 123, Train Acc: 0.9847856154910097\n",
      "Epoch: 124, Train Acc: 0.9847856154910097\n",
      "Epoch: 125, Train Acc: 0.9847856154910097\n",
      "Epoch: 126, Train Acc: 0.9847856154910097\n",
      "Epoch: 127, Train Acc: 0.9847856154910097\n",
      "Epoch: 128, Train Acc: 0.9847856154910097\n",
      "Epoch: 129, Train Acc: 0.9861687413554634\n",
      "Epoch: 130, Train Acc: 0.9861687413554634\n",
      "Epoch: 131, Train Acc: 0.9861687413554634\n",
      "Epoch: 132, Train Acc: 0.9861687413554634\n",
      "Epoch: 133, Train Acc: 0.9861687413554634\n",
      "Epoch: 134, Train Acc: 0.9861687413554634\n",
      "Epoch: 135, Train Acc: 0.9875518672199171\n",
      "Epoch: 136, Train Acc: 0.9875518672199171\n",
      "Epoch: 137, Train Acc: 0.9889349930843707\n",
      "Epoch: 138, Train Acc: 0.9903181189488244\n",
      "Epoch: 139, Train Acc: 0.9903181189488244\n",
      "Epoch: 140, Train Acc: 0.9903181189488244\n",
      "Epoch: 141, Train Acc: 0.9903181189488244\n",
      "Epoch: 142, Train Acc: 0.9903181189488244\n",
      "Epoch: 143, Train Acc: 0.9903181189488244\n",
      "Epoch: 144, Train Acc: 0.9903181189488244\n",
      "Epoch: 145, Train Acc: 0.9903181189488244\n",
      "Epoch: 146, Train Acc: 0.9903181189488244\n",
      "Epoch: 147, Train Acc: 0.9903181189488244\n",
      "Epoch: 148, Train Acc: 0.991701244813278\n",
      "Epoch: 149, Train Acc: 0.991701244813278\n",
      "Epoch: 150, Train Acc: 0.991701244813278\n",
      "Epoch: 151, Train Acc: 0.991701244813278\n",
      "Epoch: 152, Train Acc: 0.991701244813278\n",
      "Epoch: 153, Train Acc: 0.991701244813278\n",
      "Epoch: 154, Train Acc: 0.991701244813278\n",
      "Epoch: 155, Train Acc: 0.991701244813278\n",
      "Epoch: 156, Train Acc: 0.991701244813278\n",
      "Epoch: 157, Train Acc: 0.991701244813278\n",
      "Epoch: 158, Train Acc: 0.991701244813278\n",
      "Epoch: 159, Train Acc: 0.991701244813278\n",
      "Epoch: 160, Train Acc: 0.991701244813278\n",
      "Epoch: 161, Train Acc: 0.991701244813278\n",
      "Epoch: 162, Train Acc: 0.991701244813278\n",
      "Epoch: 163, Train Acc: 0.9930843706777317\n",
      "Epoch: 164, Train Acc: 0.9930843706777317\n",
      "Epoch: 165, Train Acc: 0.9930843706777317\n",
      "Epoch: 166, Train Acc: 0.9930843706777317\n",
      "Epoch: 167, Train Acc: 0.9930843706777317\n",
      "Epoch: 168, Train Acc: 0.9944674965421854\n",
      "Epoch: 169, Train Acc: 0.9944674965421854\n",
      "Epoch: 170, Train Acc: 0.9944674965421854\n",
      "Epoch: 171, Train Acc: 0.9944674965421854\n",
      "Epoch: 172, Train Acc: 0.9944674965421854\n",
      "Epoch: 173, Train Acc: 0.9944674965421854\n",
      "Epoch: 174, Train Acc: 0.9944674965421854\n",
      "Epoch: 175, Train Acc: 0.9944674965421854\n",
      "Epoch: 176, Train Acc: 0.9944674965421854\n",
      "Epoch: 177, Train Acc: 0.9944674965421854\n",
      "Epoch: 178, Train Acc: 0.9944674965421854\n",
      "Epoch: 179, Train Acc: 0.9944674965421854\n",
      "Epoch: 180, Train Acc: 0.9944674965421854\n",
      "Epoch: 181, Train Acc: 0.9944674965421854\n",
      "Epoch: 182, Train Acc: 0.9944674965421854\n",
      "Epoch: 183, Train Acc: 0.9944674965421854\n",
      "Epoch: 184, Train Acc: 0.9944674965421854\n",
      "Epoch: 185, Train Acc: 0.995850622406639\n",
      "Epoch: 186, Train Acc: 0.995850622406639\n",
      "Epoch: 187, Train Acc: 0.995850622406639\n",
      "Epoch: 188, Train Acc: 0.995850622406639\n",
      "Epoch: 189, Train Acc: 0.995850622406639\n",
      "Epoch: 190, Train Acc: 0.995850622406639\n",
      "Epoch: 191, Train Acc: 0.995850622406639\n",
      "Epoch: 192, Train Acc: 0.995850622406639\n",
      "Epoch: 193, Train Acc: 0.995850622406639\n",
      "Epoch: 194, Train Acc: 0.995850622406639\n",
      "Epoch: 195, Train Acc: 0.995850622406639\n",
      "Epoch: 196, Train Acc: 0.995850622406639\n",
      "Epoch: 197, Train Acc: 0.995850622406639\n",
      "Epoch: 198, Train Acc: 0.995850622406639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 199, Train Acc: 0.9972337482710927\n",
      "Epoch: 200, Train Acc: 0.9972337482710927\n",
      "Epoch: 201, Train Acc: 0.9972337482710927\n",
      "Epoch: 202, Train Acc: 0.9972337482710927\n",
      "Epoch: 203, Train Acc: 0.9972337482710927\n",
      "Epoch: 204, Train Acc: 0.9972337482710927\n",
      "Epoch: 205, Train Acc: 0.9972337482710927\n",
      "Epoch: 206, Train Acc: 0.9972337482710927\n",
      "Epoch: 207, Train Acc: 0.9972337482710927\n",
      "Epoch: 208, Train Acc: 0.9972337482710927\n",
      "Epoch: 209, Train Acc: 0.9972337482710927\n",
      "Epoch: 210, Train Acc: 0.9972337482710927\n",
      "Epoch: 211, Train Acc: 0.9972337482710927\n",
      "Epoch: 212, Train Acc: 0.9972337482710927\n",
      "Epoch: 213, Train Acc: 0.9972337482710927\n",
      "Epoch: 214, Train Acc: 0.9972337482710927\n",
      "Epoch: 215, Train Acc: 0.9972337482710927\n",
      "Epoch: 216, Train Acc: 0.9972337482710927\n",
      "Epoch: 217, Train Acc: 0.9972337482710927\n",
      "Epoch: 218, Train Acc: 0.9972337482710927\n",
      "Epoch: 219, Train Acc: 0.9972337482710927\n",
      "Epoch: 220, Train Acc: 0.9986168741355463\n",
      "Epoch: 221, Train Acc: 0.9986168741355463\n",
      "Epoch: 222, Train Acc: 0.9986168741355463\n",
      "Epoch: 223, Train Acc: 0.9986168741355463\n",
      "Epoch: 224, Train Acc: 0.9986168741355463\n",
      "Epoch: 225, Train Acc: 0.9986168741355463\n",
      "Epoch: 226, Train Acc: 0.9986168741355463\n",
      "Epoch: 227, Train Acc: 0.9986168741355463\n",
      "Epoch: 228, Train Acc: 0.9986168741355463\n",
      "Epoch: 229, Train Acc: 0.9986168741355463\n",
      "Epoch: 230, Train Acc: 0.9986168741355463\n",
      "Epoch: 231, Train Acc: 0.9986168741355463\n",
      "Epoch: 232, Train Acc: 0.9986168741355463\n",
      "Epoch: 233, Train Acc: 0.9986168741355463\n",
      "Epoch: 234, Train Acc: 0.9986168741355463\n",
      "Epoch: 235, Train Acc: 0.9986168741355463\n",
      "Epoch: 236, Train Acc: 0.9986168741355463\n",
      "Epoch: 237, Train Acc: 0.9986168741355463\n",
      "Epoch: 238, Train Acc: 0.9986168741355463\n",
      "Epoch: 239, Train Acc: 0.9986168741355463\n",
      "Epoch: 240, Train Acc: 0.9986168741355463\n",
      "Epoch: 241, Train Acc: 0.9986168741355463\n",
      "Epoch: 242, Train Acc: 0.9986168741355463\n",
      "Epoch: 243, Train Acc: 1.0\n",
      "722\n",
      "Number of test graphs: 722\n",
      "[['\"SRR1810239\"' '\"Liver\"' '6' ... '0.0015594061696901917'\n",
      "  '8.752765552344499e-07' '0.015814704820513725']\n",
      " ['\"SRR7161462\"' '\"Intestine\"' '4' ... '7.735918643447803e-07'\n",
      "  '0.0003169628034811467' '3.482377223917865e-06']\n",
      " ['\"SRR2033443\"' '\"Liver\"' '6' ... '8.891610377759207e-06'\n",
      "  '9.558434044265596e-09' '8.240556780947372e-06']\n",
      " ...\n",
      " ['\"ERR2704934\"' '\"Liver\"' '6' ... '0.00028185732662677765'\n",
      "  '0.004213389940559864' '3.447160997893661e-05']\n",
      " ['\"SRR5277035\"' '\"Brain\"' '1' ... '0.0004832936974707991'\n",
      "  '7.56079572283852e-08' '9.556173608871177e-06']\n",
      " ['\"SRR8857561\"' '\"Adipose\"' '0' ... '2.5084551680265577e-07'\n",
      "  '2.064464510453945e-08' '3.1962874345481396e-05']]\n",
      "ari: 0.8284073201123906\n",
      "test_ari: 0.8284073201123906\n",
      "model saved as /mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/GEO_model_validation/GNN/tuned_pytorch_GEO_model_validation_resnet_model.pt\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc.weight.requires_grad = True\n",
    "model.fc.bias.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "data_list = build_resnet_datalist(node_features_fn, graph_targets_fn, sampleID_fn)\n",
    "print(len(data_list))\n",
    "# retrain model for fine tuning transfer learning\n",
    "train_data_list = data_list[0::2]\n",
    "print(len(train_data_list))\n",
    "print(f'Number of training graphs: {len(train_data_list)}')\n",
    "train_data_loader = build_reactome_graph_loader(train_data_list, BATCH_SIZE)\n",
    "for epoch in range(EPOCHS):\n",
    "    train(train_data_loader, device)\n",
    "    train_acc = train(train_data_loader, device)\n",
    "    print(f'Epoch: {epoch}, Train Acc: {train_acc}')\n",
    "    if train_acc == 1.0:\n",
    "        break\n",
    "\n",
    "test_data_list = data_list[1::2]\n",
    "print(len(test_data_list))\n",
    "print(f'Number of test graphs: {len(test_data_list)}')\n",
    "\n",
    "test_data_loader = build_reactome_graph_loader(test_data_list, BATCH_SIZE)\n",
    "test_ari = test(test_data_loader, device)\n",
    "print(f'test_ari: {test_ari}')\n",
    "\n",
    "model_save_name = f'tuned_pytorch_GEO_model_validation_resnet_model.pt'\n",
    "path = f'/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/GEO_model_validation/GNN/{model_save_name}'\n",
    "torch.save(model.state_dict(), path)\n",
    "print(f'model saved as {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2d598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9126c3cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
