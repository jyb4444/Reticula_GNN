{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f2e3dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import random\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn.functional as nn_func\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GraphConv, global_mean_pool\n",
    "\n",
    "random.seed = 88888888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a2ca0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features exist: True, targets exist: True, edges exist: True\n"
     ]
    }
   ],
   "source": [
    "edges_fn = '/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/GEO_model_training/input_summation/edges.txt'\n",
    "node_features_fn = '/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/GEO_model_training/input_summation/node_features.txt'\n",
    "graph_targets_fn = '/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/GEO_model_training/input_summation/graph_targets.txt'\n",
    "output_fn = '/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/GEO_model_training/output/gnn_predictions_summation.tsv'\n",
    "\n",
    "features_exist = op.exists(node_features_fn)\n",
    "targets_exist = op.exists(graph_targets_fn)\n",
    "edges_exist = op.exists(edges_fn)\n",
    "\n",
    "print(f'features exist: {features_exist},'\n",
    "      f' targets exist: {targets_exist},'\n",
    "      f' edges exist: {edges_exist}')\n",
    "assert features_exist\n",
    "assert targets_exist\n",
    "assert edges_exist\n",
    "\n",
    "# magic numbers\n",
    "INPUT_CHANNELS = 1\n",
    "OUTPUT_CHANNELS = 26\n",
    "HIDDEN_CHANNELS = 64\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 500 #set this to 200 - 2000\n",
    "BENCHMARKING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3719347",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.conv1 = GraphConv(INPUT_CHANNELS, hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, OUTPUT_CHANNELS)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_weight=None):\n",
    "        # 1. Obtain node embeddings\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = nn_func.dropout(x, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def read_reactome_graph(e_fn):\n",
    "    e_v1 = []\n",
    "    e_v2 = []\n",
    "\n",
    "    for line in open(e_fn, 'r'):\n",
    "        dt = line.split()\n",
    "        node1 = int(dt[0]) - 1  # subtracting to convert R idx to python idx\n",
    "        node2 = int(dt[1]) - 1  # \" \"\n",
    "        e_v1.append(node1)\n",
    "        e_v2.append(node2)\n",
    "\n",
    "    return e_v1, e_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fd12e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_reactome_graph_datalist(e_v1, e_v2, n_fn, g_fn):\n",
    "    edge_index = torch.tensor([e_v1, e_v2], dtype=torch.long)\n",
    "    feature_v = numpy.loadtxt(n_fn)\n",
    "    target_v = numpy.loadtxt(g_fn, dtype=str, delimiter=\",\")\n",
    "\n",
    "    target_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "    target_v = target_encoder.fit_transform(target_v)\n",
    "\n",
    "    d_list = []\n",
    "    for row_idx in range(len(feature_v)):\n",
    "        features = feature_v[row_idx, :]\n",
    "        x = torch.tensor(features, dtype=torch.float)\n",
    "        x = x.unsqueeze(1)\n",
    "        y = torch.tensor([target_v[row_idx]])\n",
    "        d_list.append(Data(x=x, y=y, edge_index=edge_index))\n",
    "\n",
    "    return d_list\n",
    "\n",
    "\n",
    "def build_reactome_graph_loader(d_list, batch_size):\n",
    "    loader = DataLoader(d_list, batch_size=batch_size, shuffle=False)#True)\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "def train(loader, dv):\n",
    "    model.train()\n",
    "\n",
    "    correct = 0\n",
    "    for batch in loader:  # Iterate in batches over the training dataset.\n",
    "        x = batch.x.to(dv)\n",
    "        e = batch.edge_index.to(dv)\n",
    "        b = batch.batch.to(dv)\n",
    "        y = batch.y.to(dv)\n",
    "        out = model(x, e, b)  # Perform a single forward pass.\n",
    "        loss = criterion(out, y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == y).sum())  # Check against ground-truth labels.\n",
    "    return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "def test(loader, dv):\n",
    "    model.eval()\n",
    "\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    for batch in loader:  # Iterate in batches over the test dataset.\n",
    "        x = batch.x.to(dv)\n",
    "        e = batch.edge_index.to(dv)\n",
    "        b = batch.batch.to(dv)\n",
    "        y = batch.y.to(dv)\n",
    "        targets += torch.Tensor.tolist(y)\n",
    "        out = model(x, e, b)  # Perform a single forward pass.\n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        predictions += torch.Tensor.tolist(pred)\n",
    "    print(targets)\n",
    "    print(predictions)\n",
    "    numpy.savetxt(output_fn, numpy.transpose([targets, predictions]),\n",
    "                  fmt='%d', delimiter='\\t', header='target\\tprediction')\n",
    "    ari = adjusted_rand_score(targets, predictions)\n",
    "    print(f'ari: {ari}')\n",
    "    return ari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1b2a08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6295\n",
      "6295\n",
      "Number of training graphs: 6295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/yuankeji/anaconda3/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Acc: 0.22081016679904686\n",
      "Epoch: 1, Train Acc: 0.24463860206513105\n",
      "Epoch: 2, Train Acc: 0.2581413820492454\n",
      "Epoch: 3, Train Acc: 0.264813343923749\n",
      "Epoch: 4, Train Acc: 0.2681493248610008\n",
      "Epoch: 5, Train Acc: 0.27768069896743447\n",
      "Epoch: 6, Train Acc: 0.28228752978554406\n",
      "Epoch: 7, Train Acc: 0.2942017474185862\n",
      "Epoch: 8, Train Acc: 0.29864972200158857\n",
      "Epoch: 9, Train Acc: 0.30325655281969816\n",
      "Epoch: 10, Train Acc: 0.30945194598888004\n",
      "Epoch: 11, Train Acc: 0.3173947577442415\n",
      "Epoch: 12, Train Acc: 0.32867355043685464\n",
      "Epoch: 13, Train Acc: 0.33804606830818107\n",
      "Epoch: 14, Train Acc: 0.35409054805401113\n",
      "Epoch: 15, Train Acc: 0.356791104050834\n",
      "Epoch: 16, Train Acc: 0.3787132644956315\n",
      "Epoch: 17, Train Acc: 0.3925337569499603\n",
      "Epoch: 18, Train Acc: 0.40365369340746626\n",
      "Epoch: 19, Train Acc: 0.40778395552025415\n",
      "Epoch: 20, Train Acc: 0.4103256552819698\n",
      "Epoch: 21, Train Acc: 0.42748212867355045\n",
      "Epoch: 22, Train Acc: 0.42875297855440825\n",
      "Epoch: 23, Train Acc: 0.43272438443208894\n",
      "Epoch: 24, Train Acc: 0.4443208895949166\n",
      "Epoch: 25, Train Acc: 0.4403494837172359\n",
      "Epoch: 26, Train Acc: 0.4438443208895949\n",
      "Epoch: 27, Train Acc: 0.4476568705321684\n",
      "Epoch: 28, Train Acc: 0.44972200158856235\n",
      "Epoch: 29, Train Acc: 0.45131056393963465\n",
      "Epoch: 30, Train Acc: 0.4540111199364575\n",
      "Epoch: 31, Train Acc: 0.459253375694996\n",
      "Epoch: 32, Train Acc: 0.45909451945988883\n",
      "Epoch: 33, Train Acc: 0.45544082605242253\n",
      "Epoch: 34, Train Acc: 0.46274821286735507\n",
      "Epoch: 35, Train Acc: 0.46386020651310567\n",
      "Epoch: 36, Train Acc: 0.4671961874503574\n",
      "Epoch: 37, Train Acc: 0.46973788721207305\n",
      "Epoch: 38, Train Acc: 0.47084988085782364\n",
      "Epoch: 39, Train Acc: 0.47084988085782364\n",
      "Epoch: 40, Train Acc: 0.4748212867355044\n",
      "Epoch: 41, Train Acc: 0.475933280381255\n",
      "Epoch: 42, Train Acc: 0.47577442414614773\n",
      "Epoch: 43, Train Acc: 0.4792692613185068\n",
      "Epoch: 44, Train Acc: 0.48117553613979347\n",
      "Epoch: 45, Train Acc: 0.48625893566322476\n",
      "Epoch: 46, Train Acc: 0.4886417791898332\n",
      "Epoch: 47, Train Acc: 0.4902303415409055\n",
      "Epoch: 48, Train Acc: 0.48721207307386816\n",
      "Epoch: 49, Train Acc: 0.49674344718030183\n",
      "Epoch: 50, Train Acc: 0.49293089753772834\n",
      "Epoch: 51, Train Acc: 0.5029388403494837\n",
      "Epoch: 52, Train Acc: 0.49722001588562353\n",
      "Epoch: 53, Train Acc: 0.5099285146942018\n",
      "Epoch: 54, Train Acc: 0.5034154090548054\n",
      "Epoch: 55, Train Acc: 0.5104050833995234\n",
      "Epoch: 56, Train Acc: 0.5131056393963463\n",
      "Epoch: 57, Train Acc: 0.5123113582208102\n",
      "Epoch: 58, Train Acc: 0.5189833200953138\n",
      "Epoch: 59, Train Acc: 0.5146942017474185\n",
      "Epoch: 60, Train Acc: 0.5180301826846704\n",
      "Epoch: 61, Train Acc: 0.524702144559174\n",
      "Epoch: 62, Train Acc: 0.5278792692613186\n",
      "Epoch: 63, Train Acc: 0.521207307386815\n",
      "Epoch: 64, Train Acc: 0.5320095313741064\n",
      "Epoch: 65, Train Acc: 0.5172359015091342\n",
      "Epoch: 66, Train Acc: 0.5301032565528196\n",
      "Epoch: 67, Train Acc: 0.5289912629070691\n",
      "Epoch: 68, Train Acc: 0.5283558379666402\n",
      "Epoch: 69, Train Acc: 0.5366163621922161\n"
     ]
    }
   ],
   "source": [
    "def change_key(self, old, new):\n",
    "    for _ in range(len(self)):\n",
    "        k, v = self.popitem(False)\n",
    "        self[new if old == k else k] = v\n",
    "\n",
    "\n",
    "(edge_v1, edge_v2) = read_reactome_graph(edges_fn)\n",
    "model = GNN(hidden_channels=HIDDEN_CHANNELS)\n",
    "device = cpu = torch.device('cpu')\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "acc_str = ''\n",
    "\n",
    "data_list = build_reactome_graph_datalist(edge_v1, edge_v2, node_features_fn, graph_targets_fn)\n",
    "print(len(data_list))\n",
    "# retrain model for fine tuning transfer learning\n",
    "train_data_list = data_list\n",
    "print(len(train_data_list))\n",
    "print(f'Number of training graphs: {len(train_data_list)}')\n",
    "train_data_loader = build_reactome_graph_loader(train_data_list, BATCH_SIZE)\n",
    "for epoch in range(EPOCHS):\n",
    "    #print(f'epoch loop')\n",
    "    train_acc = train(train_data_loader, device)\n",
    "    print(f'Epoch: {epoch}, Train Acc: {train_acc}')\n",
    "    acc_str += f'{train_acc:.4f}'#',{test_acc:.4f}\\n'\n",
    "    if train_acc == 1.0:\n",
    "        break\n",
    "        \n",
    "training_acc_fn = F\"summation_graph_classification_acc_full_dataset.txt\"\n",
    "path = F\"/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/GEO_model_training/GNN/Summation/{training_acc_fn}\"\n",
    "with open(path, 'w') as writefile:\n",
    "  writefile.write(acc_str)\n",
    "\n",
    "test_data_list = data_list\n",
    "print(len(test_data_list))\n",
    "print(f'Number of test graphs: {len(test_data_list)}')\n",
    "\n",
    "test_data_loader = build_reactome_graph_loader(test_data_list, BATCH_SIZE)\n",
    "test_ari = test(test_data_loader, device)\n",
    "print(f'test_ari: {test_ari}')\n",
    "\n",
    "model_save_name = f'summation_fully_trained_pytorch_GEO_model_training_gnn_model.pt'\n",
    "path = f'/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/GEO_model_training/GNN/Summation/{model_save_name}'\n",
    "torch.save(model.state_dict(), path)\n",
    "print(f'model saved as {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc11fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b1a6db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
