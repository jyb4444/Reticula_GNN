{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69904162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import random\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn.functional as nn_func\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GraphConv, global_mean_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bb98f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features exist: True, targets exist: True, edges exist: True  model exists: True\n"
     ]
    }
   ],
   "source": [
    "node_features_fn = '/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/input/node_features0.03vs0_2_time_cross.txt'\n",
    "graph_targets_fn = '/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/input/graph_targets0.03vs0_time_cross.txt'\n",
    "edges_fn = '/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/GEO_model_training/input/edges.txt'\n",
    "model_fn = '/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/GEO_model_training/GNN/trained_pytorch_model_rewired10_fold_full_dataset.pt'\n",
    "output_fn = '/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/output/predictions0.03vs0_time_cross.tsv'\n",
    "sample_id_fn = '/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/output/tcdd_sample_id0.03vs0_time_cross.txt'\n",
    "project_id_fn = '/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/output/tcdd_project_id0.03vs0_time_cross.txt'\n",
    "gender_fn = '/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/output/tcdd_gender0.03vs0_time_cross.txt'\n",
    "dose_fn = '/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/output/tcdd_dose0.03vs0_time_cross.txt'\n",
    "# test graph_targets.txt, node_features.txt and edges.txt\n",
    "features_exist = op.exists(node_features_fn)\n",
    "targets_exist = op.exists(graph_targets_fn)\n",
    "edges_exist = op.exists(edges_fn)\n",
    "model_exists = op.exists(model_fn)\n",
    "\n",
    "print(f'features exist: {features_exist},'\n",
    "      f' targets exist: {targets_exist},'\n",
    "      f' edges exist: {edges_exist}',\n",
    "      f' model exists: {model_exists}')\n",
    "assert features_exist\n",
    "assert targets_exist\n",
    "assert edges_exist\n",
    "assert model_exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0469e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CHANNELS = 1\n",
    "OUTPUT_CHANNELS = 26\n",
    "NEW_CHANNELS = 2\n",
    "HIDDEN_CHANNELS = 64\n",
    "BATCH_SIZE = 64\n",
    "BENCHMARKING = False\n",
    "EPOCHS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e671b274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.conv1 = GraphConv(INPUT_CHANNELS, hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, OUTPUT_CHANNELS)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_weight=None):\n",
    "        # 1. Obtain node embeddings\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def read_reactome_graph(e_fn):\n",
    "    e_v1 = []\n",
    "    e_v2 = []\n",
    "\n",
    "    for line in open(e_fn, 'r'):\n",
    "        dt = line.split()\n",
    "        node1 = int(dt[0]) - 1  # subtracting to convert R idx to python idx\n",
    "        node2 = int(dt[1]) - 1  # \" \"\n",
    "        e_v1.append(node1)\n",
    "        e_v2.append(node2)\n",
    "\n",
    "    return e_v1, e_v2\n",
    "\n",
    "\n",
    "def build_reactome_graph_datalist(e_v1, e_v2, n_fn, g_fn, pid_fn, sid_fn, gen_fn, dose_fn):\n",
    "    edge_index = torch.tensor([e_v1, e_v2], dtype=torch.long)\n",
    "    feature_v = numpy.loadtxt(n_fn)\n",
    "    target_v = numpy.loadtxt(g_fn, dtype=float, delimiter=\",\")\n",
    "    projectID_v = numpy.loadtxt(pid_fn, dtype=str, delimiter=\"\\t\")\n",
    "    sampleID_v = numpy.loadtxt(sid_fn, dtype=str, delimiter=\"\\t\")\n",
    "    gender_v = numpy.loadtxt(gen_fn, dtype=str, delimiter=\"\\t\")\n",
    "    dose_v = numpy.loadtxt(dose_fn, dtype=str, delimiter=\"\\t\")\n",
    "    \n",
    "    binary_labels = (target_v > 0).astype(int)\n",
    "    \n",
    "    print(\"labels check:\")\n",
    "    for dose, label in zip(target_v[:10], binary_labels[:10]): \n",
    "        print(f\"dose: {dose}, label: {label}\")\n",
    "\n",
    "\n",
    "    d_list = []\n",
    "    for row_idx in range(len(feature_v)):\n",
    "        features = feature_v[row_idx, :]\n",
    "        x = torch.tensor(features, dtype=torch.float)\n",
    "        x = x.unsqueeze(1)\n",
    "#         y = torch.tensor([target_v[row_idx]])\n",
    "        y = torch.tensor([binary_labels[row_idx]], dtype=torch.long)\n",
    "        \n",
    "        pid = projectID_v[row_idx]\n",
    "        sid = sampleID_v[row_idx]\n",
    "        gen = gender_v[row_idx]\n",
    "        dose = dose_v[row_idx]\n",
    "        \n",
    "        d_list.append(Data(x=x, y=y, pid=pid, sid=sid, gen=gen, dose=dose, edge_index=edge_index))\n",
    "\n",
    "    return d_list\n",
    "\n",
    "\n",
    "def build_reactome_graph_loader(d_list, batch_size):\n",
    "    loader = DataLoader(d_list, batch_size=batch_size, shuffle=False)  # True)\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "def train(loader, dv):\n",
    "    model.train()\n",
    "\n",
    "    correct = 0\n",
    "    for batch in loader:  # Iterate in batches over the training dataset.\n",
    "        batch.validate()\n",
    "        x = batch.x.to(dv)\n",
    "        e = batch.edge_index.to(dv)\n",
    "        b = batch.batch.to(dv)\n",
    "        y = batch.y.to(dv)\n",
    "\n",
    "        out = model(x, e, b)  # Perform a single forward pass.\n",
    "        loss = criterion(out, y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == y).sum())  # Check against ground-truth labels.\n",
    "    return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "def test(loader, dv):\n",
    "    model.eval()\n",
    "\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    project_ids = []\n",
    "    sample_ids = []\n",
    "    genders = []\n",
    "    doses = []\n",
    "    confidences = []\n",
    "    for batch in loader:  # Iterate in batches over the test dataset.\n",
    "        x = batch.x.to(dv)\n",
    "        e = batch.edge_index.to(dv)\n",
    "        b = batch.batch.to(dv)\n",
    "        y = batch.y.to(dv)\n",
    "        targets += torch.Tensor.tolist(y)\n",
    "        \n",
    "        project_ids += batch.pid\n",
    "        sample_ids += batch.sid\n",
    "        genders += batch.gen\n",
    "        doses += batch.dose\n",
    "        \n",
    "        out = model(x, e, b)  # Perform a single forward pass.\n",
    "        prob = torch.softmax(out, dim=1)\n",
    "        \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        predictions += torch.Tensor.tolist(pred)\n",
    "        confidences += torch.Tensor.tolist(prob)\n",
    "        \n",
    "    num_classes = len(confidences[0])\n",
    "\n",
    "    data_to_save = []\n",
    "    for i in range(len(targets)):\n",
    "        row = [project_ids[i], sample_ids[i], genders[i], doses[i], targets[i], predictions[i]] + confidences[i]\n",
    "        data_to_save.append(row)\n",
    "    data_to_save = numpy.array(data_to_save)\n",
    "    print(data_to_save)\n",
    "    \n",
    "    fmt = ['%s', '%s', '%s', '%s', '%s', '%s'] + ['%s' for _ in range(num_classes)]\n",
    "    \n",
    "    headers = ['project_ids', 'sample_ids', 'genders', 'doses', 'target', 'prediction'] + [f'confidence_class_{i}' for i in range(num_classes)]\n",
    "    numpy.savetxt(output_fn, data_to_save, fmt='\\t'.join(fmt), delimiter='\\t', header='\\t'.join(headers), comments='')\n",
    "        \n",
    "    ari = adjusted_rand_score(targets, predictions)\n",
    "    print(f'ari: {ari}')\n",
    "    return ari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "233d9261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNN(\n",
       "  (conv1): GraphConv(1, 64)\n",
       "  (conv2): GraphConv(64, 64)\n",
       "  (conv3): GraphConv(64, 64)\n",
       "  (lin): Linear(in_features=64, out_features=26, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def change_key(self, old, new):\n",
    "    for _ in range(len(self)):\n",
    "        k, v = self.popitem(False)\n",
    "        self[new if old == k else k] = v\n",
    "\n",
    "\n",
    "(edge_v1, edge_v2) = read_reactome_graph(edges_fn)\n",
    "model = GNN(hidden_channels=HIDDEN_CHANNELS)\n",
    "device = cpu = torch.device('cpu')\n",
    "\n",
    "sd = torch.load(model_fn, map_location=device)\n",
    "change_key(sd, 'conv1.lin_l.weight', 'conv1.lin_rel.weight')\n",
    "change_key(sd, 'conv1.lin_l.bias', 'conv1.lin_rel.bias')\n",
    "change_key(sd, 'conv1.lin_r.weight', 'conv1.lin_root.weight')\n",
    "change_key(sd, 'conv2.lin_l.weight', 'conv2.lin_rel.weight')\n",
    "change_key(sd, 'conv2.lin_l.bias', 'conv2.lin_rel.bias')\n",
    "change_key(sd, 'conv2.lin_r.weight', 'conv2.lin_root.weight')\n",
    "change_key(sd, 'conv3.lin_l.weight', 'conv3.lin_rel.weight')\n",
    "change_key(sd, 'conv3.lin_l.bias', 'conv3.lin_rel.bias')\n",
    "change_key(sd, 'conv3.lin_r.weight', 'conv3.lin_root.weight')\n",
    "change_key(sd, 'lin.weight', 'lin.weight')\n",
    "change_key(sd, 'lin.bias', 'lin.bias')\n",
    "\n",
    "model.load_state_dict(sd)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9c203f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace final layer with new shape matching new dataset\n",
    "model.lin = Linear(HIDDEN_CHANNELS, NEW_CHANNELS)\n",
    "\n",
    "model.conv1.lin_rel.weight.requires_grad = False\n",
    "model.conv1.lin_rel.bias.requires_grad = False\n",
    "model.conv1.lin_root.weight.requires_grad = False\n",
    "model.conv2.lin_rel.weight.requires_grad = False\n",
    "model.conv2.lin_rel.bias.requires_grad = False\n",
    "model.conv2.lin_root.weight.requires_grad = False\n",
    "model.conv3.lin_rel.weight.requires_grad = False\n",
    "model.conv3.lin_rel.bias.requires_grad = False\n",
    "model.conv3.lin_root.weight.requires_grad = False\n",
    "model.lin.weight.requires_grad = True\n",
    "model.lin.bias.requires_grad = True\n",
    "\n",
    "# for name, param in model.named_parameters(): print(name, param)\n",
    "\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8184a53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels check:\n",
      "dose: 0.0, label: 0\n",
      "dose: 0.0, label: 0\n",
      "dose: 0.0, label: 0\n",
      "dose: 0.0, label: 0\n",
      "dose: 0.0, label: 0\n",
      "dose: 0.0, label: 0\n",
      "dose: 0.0, label: 0\n",
      "dose: 0.0, label: 0\n",
      "dose: 0.0, label: 0\n",
      "dose: 0.0, label: 0\n",
      "91\n",
      "91\n",
      "Number of training graphs: 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/yuankeji/anaconda3/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/mnt/home/yuankeji/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Acc: 0.31868131868131866\n",
      "Epoch: 1, Train Acc: 0.4835164835164835\n",
      "Epoch: 2, Train Acc: 0.5714285714285714\n",
      "Epoch: 3, Train Acc: 0.6373626373626373\n",
      "Epoch: 4, Train Acc: 0.6923076923076923\n",
      "Epoch: 5, Train Acc: 0.6923076923076923\n",
      "Epoch: 6, Train Acc: 0.7032967032967034\n",
      "Epoch: 7, Train Acc: 0.7252747252747253\n",
      "Epoch: 8, Train Acc: 0.7252747252747253\n",
      "Epoch: 9, Train Acc: 0.7252747252747253\n",
      "Epoch: 10, Train Acc: 0.7252747252747253\n",
      "Epoch: 11, Train Acc: 0.7252747252747253\n",
      "Epoch: 12, Train Acc: 0.7362637362637363\n",
      "Epoch: 13, Train Acc: 0.7362637362637363\n",
      "Epoch: 14, Train Acc: 0.7362637362637363\n",
      "Epoch: 15, Train Acc: 0.7472527472527473\n",
      "Epoch: 16, Train Acc: 0.7472527472527473\n",
      "Epoch: 17, Train Acc: 0.7472527472527473\n",
      "Epoch: 18, Train Acc: 0.7472527472527473\n",
      "Epoch: 19, Train Acc: 0.7472527472527473\n",
      "Epoch: 20, Train Acc: 0.7472527472527473\n",
      "Epoch: 21, Train Acc: 0.7362637362637363\n",
      "Epoch: 22, Train Acc: 0.7472527472527473\n",
      "Epoch: 23, Train Acc: 0.7472527472527473\n",
      "Epoch: 24, Train Acc: 0.7472527472527473\n",
      "Epoch: 25, Train Acc: 0.7472527472527473\n",
      "Epoch: 26, Train Acc: 0.7472527472527473\n",
      "Epoch: 27, Train Acc: 0.7472527472527473\n",
      "Epoch: 28, Train Acc: 0.7472527472527473\n",
      "Epoch: 29, Train Acc: 0.7472527472527473\n",
      "Epoch: 30, Train Acc: 0.7472527472527473\n",
      "Epoch: 31, Train Acc: 0.7472527472527473\n",
      "Epoch: 32, Train Acc: 0.7472527472527473\n",
      "Epoch: 33, Train Acc: 0.7472527472527473\n",
      "Epoch: 34, Train Acc: 0.7472527472527473\n",
      "Epoch: 35, Train Acc: 0.7472527472527473\n",
      "Epoch: 36, Train Acc: 0.7472527472527473\n",
      "Epoch: 37, Train Acc: 0.7472527472527473\n",
      "Epoch: 38, Train Acc: 0.7472527472527473\n",
      "Epoch: 39, Train Acc: 0.7472527472527473\n",
      "Epoch: 40, Train Acc: 0.7472527472527473\n",
      "Epoch: 41, Train Acc: 0.7472527472527473\n",
      "Epoch: 42, Train Acc: 0.7472527472527473\n",
      "Epoch: 43, Train Acc: 0.7472527472527473\n",
      "Epoch: 44, Train Acc: 0.7472527472527473\n",
      "Epoch: 45, Train Acc: 0.7472527472527473\n",
      "Epoch: 46, Train Acc: 0.7472527472527473\n",
      "Epoch: 47, Train Acc: 0.7472527472527473\n",
      "Epoch: 48, Train Acc: 0.7472527472527473\n",
      "Epoch: 49, Train Acc: 0.7472527472527473\n",
      "Epoch: 50, Train Acc: 0.7472527472527473\n",
      "Epoch: 51, Train Acc: 0.7472527472527473\n",
      "Epoch: 52, Train Acc: 0.7472527472527473\n",
      "Epoch: 53, Train Acc: 0.7472527472527473\n",
      "Epoch: 54, Train Acc: 0.7472527472527473\n",
      "Epoch: 55, Train Acc: 0.7472527472527473\n",
      "Epoch: 56, Train Acc: 0.7472527472527473\n",
      "Epoch: 57, Train Acc: 0.7472527472527473\n",
      "Epoch: 58, Train Acc: 0.7472527472527473\n",
      "Epoch: 59, Train Acc: 0.7472527472527473\n",
      "Epoch: 60, Train Acc: 0.7472527472527473\n",
      "Epoch: 61, Train Acc: 0.7472527472527473\n",
      "Epoch: 62, Train Acc: 0.7472527472527473\n",
      "Epoch: 63, Train Acc: 0.7472527472527473\n",
      "Epoch: 64, Train Acc: 0.7472527472527473\n",
      "Epoch: 65, Train Acc: 0.7472527472527473\n",
      "Epoch: 66, Train Acc: 0.7472527472527473\n",
      "Epoch: 67, Train Acc: 0.7472527472527473\n",
      "Epoch: 68, Train Acc: 0.7472527472527473\n",
      "Epoch: 69, Train Acc: 0.7472527472527473\n",
      "Epoch: 70, Train Acc: 0.7472527472527473\n",
      "Epoch: 71, Train Acc: 0.7472527472527473\n",
      "Epoch: 72, Train Acc: 0.7472527472527473\n",
      "Epoch: 73, Train Acc: 0.7472527472527473\n",
      "Epoch: 74, Train Acc: 0.7472527472527473\n",
      "Epoch: 75, Train Acc: 0.7472527472527473\n",
      "Epoch: 76, Train Acc: 0.7472527472527473\n",
      "Epoch: 77, Train Acc: 0.7472527472527473\n",
      "Epoch: 78, Train Acc: 0.7472527472527473\n",
      "Epoch: 79, Train Acc: 0.7472527472527473\n",
      "Epoch: 80, Train Acc: 0.7472527472527473\n",
      "Epoch: 81, Train Acc: 0.7472527472527473\n",
      "Epoch: 82, Train Acc: 0.7472527472527473\n",
      "Epoch: 83, Train Acc: 0.7472527472527473\n",
      "Epoch: 84, Train Acc: 0.7472527472527473\n",
      "Epoch: 85, Train Acc: 0.7472527472527473\n",
      "Epoch: 86, Train Acc: 0.7472527472527473\n",
      "Epoch: 87, Train Acc: 0.7472527472527473\n",
      "Epoch: 88, Train Acc: 0.7472527472527473\n",
      "Epoch: 89, Train Acc: 0.7472527472527473\n",
      "Epoch: 90, Train Acc: 0.7472527472527473\n",
      "Epoch: 91, Train Acc: 0.7472527472527473\n",
      "Epoch: 92, Train Acc: 0.7472527472527473\n",
      "Epoch: 93, Train Acc: 0.7472527472527473\n",
      "Epoch: 94, Train Acc: 0.7472527472527473\n",
      "Epoch: 95, Train Acc: 0.7472527472527473\n",
      "Epoch: 96, Train Acc: 0.7472527472527473\n",
      "Epoch: 97, Train Acc: 0.7472527472527473\n",
      "Epoch: 98, Train Acc: 0.7472527472527473\n",
      "Epoch: 99, Train Acc: 0.7472527472527473\n",
      "Epoch: 100, Train Acc: 0.7472527472527473\n",
      "Epoch: 101, Train Acc: 0.7472527472527473\n",
      "Epoch: 102, Train Acc: 0.7692307692307693\n",
      "Epoch: 103, Train Acc: 0.7692307692307693\n",
      "Epoch: 104, Train Acc: 0.7692307692307693\n",
      "Epoch: 105, Train Acc: 0.7692307692307693\n",
      "Epoch: 106, Train Acc: 0.7692307692307693\n",
      "Epoch: 107, Train Acc: 0.7692307692307693\n",
      "Epoch: 108, Train Acc: 0.7802197802197802\n",
      "Epoch: 109, Train Acc: 0.7802197802197802\n",
      "Epoch: 110, Train Acc: 0.7802197802197802\n",
      "Epoch: 111, Train Acc: 0.7802197802197802\n",
      "Epoch: 112, Train Acc: 0.7802197802197802\n",
      "Epoch: 113, Train Acc: 0.7802197802197802\n",
      "Epoch: 114, Train Acc: 0.7802197802197802\n",
      "Epoch: 115, Train Acc: 0.7802197802197802\n",
      "Epoch: 116, Train Acc: 0.7802197802197802\n",
      "Epoch: 117, Train Acc: 0.7802197802197802\n",
      "Epoch: 118, Train Acc: 0.7802197802197802\n",
      "Epoch: 119, Train Acc: 0.7802197802197802\n",
      "Epoch: 120, Train Acc: 0.7802197802197802\n",
      "Epoch: 121, Train Acc: 0.7802197802197802\n",
      "Epoch: 122, Train Acc: 0.7802197802197802\n",
      "Epoch: 123, Train Acc: 0.7802197802197802\n",
      "Epoch: 124, Train Acc: 0.7802197802197802\n",
      "Epoch: 125, Train Acc: 0.7802197802197802\n",
      "Epoch: 126, Train Acc: 0.7802197802197802\n",
      "Epoch: 127, Train Acc: 0.7802197802197802\n",
      "Epoch: 128, Train Acc: 0.7802197802197802\n",
      "Epoch: 129, Train Acc: 0.7802197802197802\n",
      "Epoch: 130, Train Acc: 0.7802197802197802\n",
      "Epoch: 131, Train Acc: 0.7802197802197802\n",
      "Epoch: 132, Train Acc: 0.7802197802197802\n",
      "Epoch: 133, Train Acc: 0.7802197802197802\n",
      "Epoch: 134, Train Acc: 0.7802197802197802\n",
      "Epoch: 135, Train Acc: 0.7802197802197802\n",
      "Epoch: 136, Train Acc: 0.7802197802197802\n",
      "Epoch: 137, Train Acc: 0.7802197802197802\n",
      "Epoch: 138, Train Acc: 0.7802197802197802\n",
      "Epoch: 139, Train Acc: 0.7802197802197802\n",
      "Epoch: 140, Train Acc: 0.7802197802197802\n",
      "Epoch: 141, Train Acc: 0.7802197802197802\n",
      "Epoch: 142, Train Acc: 0.7802197802197802\n",
      "Epoch: 143, Train Acc: 0.7802197802197802\n",
      "Epoch: 144, Train Acc: 0.7802197802197802\n",
      "Epoch: 145, Train Acc: 0.7802197802197802\n",
      "Epoch: 146, Train Acc: 0.7802197802197802\n",
      "Epoch: 147, Train Acc: 0.7802197802197802\n",
      "Epoch: 148, Train Acc: 0.7802197802197802\n",
      "Epoch: 149, Train Acc: 0.7802197802197802\n",
      "Epoch: 150, Train Acc: 0.7802197802197802\n",
      "Epoch: 151, Train Acc: 0.7802197802197802\n",
      "Epoch: 152, Train Acc: 0.7802197802197802\n",
      "Epoch: 153, Train Acc: 0.7802197802197802\n",
      "Epoch: 154, Train Acc: 0.7802197802197802\n",
      "Epoch: 155, Train Acc: 0.7802197802197802\n",
      "Epoch: 156, Train Acc: 0.7802197802197802\n",
      "Epoch: 157, Train Acc: 0.7802197802197802\n",
      "Epoch: 158, Train Acc: 0.7802197802197802\n",
      "Epoch: 159, Train Acc: 0.7802197802197802\n",
      "Epoch: 160, Train Acc: 0.7802197802197802\n",
      "Epoch: 161, Train Acc: 0.7802197802197802\n",
      "Epoch: 162, Train Acc: 0.7802197802197802\n",
      "Epoch: 163, Train Acc: 0.7802197802197802\n",
      "Epoch: 164, Train Acc: 0.7802197802197802\n",
      "Epoch: 165, Train Acc: 0.7802197802197802\n",
      "Epoch: 166, Train Acc: 0.7802197802197802\n",
      "Epoch: 167, Train Acc: 0.7802197802197802\n",
      "Epoch: 168, Train Acc: 0.7802197802197802\n",
      "Epoch: 169, Train Acc: 0.7802197802197802\n",
      "Epoch: 170, Train Acc: 0.7802197802197802\n",
      "Epoch: 171, Train Acc: 0.7802197802197802\n",
      "Epoch: 172, Train Acc: 0.7802197802197802\n",
      "Epoch: 173, Train Acc: 0.7802197802197802\n",
      "Epoch: 174, Train Acc: 0.7802197802197802\n",
      "Epoch: 175, Train Acc: 0.7802197802197802\n",
      "Epoch: 176, Train Acc: 0.7802197802197802\n",
      "Epoch: 177, Train Acc: 0.7802197802197802\n",
      "Epoch: 178, Train Acc: 0.7802197802197802\n",
      "Epoch: 179, Train Acc: 0.7802197802197802\n",
      "Epoch: 180, Train Acc: 0.7802197802197802\n",
      "Epoch: 181, Train Acc: 0.7802197802197802\n",
      "Epoch: 182, Train Acc: 0.7802197802197802\n",
      "Epoch: 183, Train Acc: 0.7802197802197802\n",
      "Epoch: 184, Train Acc: 0.7802197802197802\n",
      "Epoch: 185, Train Acc: 0.7802197802197802\n",
      "Epoch: 186, Train Acc: 0.7802197802197802\n",
      "Epoch: 187, Train Acc: 0.7802197802197802\n",
      "Epoch: 188, Train Acc: 0.7802197802197802\n",
      "Epoch: 189, Train Acc: 0.7802197802197802\n",
      "Epoch: 190, Train Acc: 0.7802197802197802\n",
      "Epoch: 191, Train Acc: 0.7802197802197802\n",
      "Epoch: 192, Train Acc: 0.7802197802197802\n",
      "Epoch: 193, Train Acc: 0.7802197802197802\n",
      "Epoch: 194, Train Acc: 0.7802197802197802\n",
      "Epoch: 195, Train Acc: 0.7802197802197802\n",
      "Epoch: 196, Train Acc: 0.7802197802197802\n",
      "Epoch: 197, Train Acc: 0.7802197802197802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 198, Train Acc: 0.7802197802197802\n",
      "Epoch: 199, Train Acc: 0.7802197802197802\n",
      "Epoch: 200, Train Acc: 0.7802197802197802\n",
      "Epoch: 201, Train Acc: 0.7802197802197802\n",
      "Epoch: 202, Train Acc: 0.7802197802197802\n",
      "Epoch: 203, Train Acc: 0.7802197802197802\n",
      "Epoch: 204, Train Acc: 0.7802197802197802\n",
      "Epoch: 205, Train Acc: 0.7802197802197802\n",
      "Epoch: 206, Train Acc: 0.7802197802197802\n",
      "Epoch: 207, Train Acc: 0.7802197802197802\n",
      "Epoch: 208, Train Acc: 0.7802197802197802\n",
      "Epoch: 209, Train Acc: 0.7802197802197802\n",
      "Epoch: 210, Train Acc: 0.7802197802197802\n",
      "Epoch: 211, Train Acc: 0.7802197802197802\n",
      "Epoch: 212, Train Acc: 0.7802197802197802\n",
      "Epoch: 213, Train Acc: 0.7802197802197802\n",
      "Epoch: 214, Train Acc: 0.7802197802197802\n",
      "Epoch: 215, Train Acc: 0.7802197802197802\n",
      "Epoch: 216, Train Acc: 0.7802197802197802\n",
      "Epoch: 217, Train Acc: 0.7802197802197802\n",
      "Epoch: 218, Train Acc: 0.7802197802197802\n",
      "Epoch: 219, Train Acc: 0.7802197802197802\n",
      "Epoch: 220, Train Acc: 0.7802197802197802\n",
      "Epoch: 221, Train Acc: 0.7802197802197802\n",
      "Epoch: 222, Train Acc: 0.7802197802197802\n",
      "Epoch: 223, Train Acc: 0.7802197802197802\n",
      "Epoch: 224, Train Acc: 0.7802197802197802\n",
      "Epoch: 225, Train Acc: 0.7802197802197802\n",
      "Epoch: 226, Train Acc: 0.7802197802197802\n",
      "Epoch: 227, Train Acc: 0.7802197802197802\n",
      "Epoch: 228, Train Acc: 0.7802197802197802\n",
      "Epoch: 229, Train Acc: 0.7802197802197802\n",
      "Epoch: 230, Train Acc: 0.7802197802197802\n",
      "Epoch: 231, Train Acc: 0.7802197802197802\n",
      "Epoch: 232, Train Acc: 0.7802197802197802\n",
      "Epoch: 233, Train Acc: 0.7802197802197802\n",
      "Epoch: 234, Train Acc: 0.7802197802197802\n",
      "Epoch: 235, Train Acc: 0.7802197802197802\n",
      "Epoch: 236, Train Acc: 0.7802197802197802\n",
      "Epoch: 237, Train Acc: 0.7802197802197802\n",
      "Epoch: 238, Train Acc: 0.7802197802197802\n",
      "Epoch: 239, Train Acc: 0.7802197802197802\n",
      "Epoch: 240, Train Acc: 0.7802197802197802\n",
      "Epoch: 241, Train Acc: 0.7802197802197802\n",
      "Epoch: 242, Train Acc: 0.7802197802197802\n",
      "Epoch: 243, Train Acc: 0.7802197802197802\n",
      "Epoch: 244, Train Acc: 0.7802197802197802\n",
      "Epoch: 245, Train Acc: 0.7802197802197802\n",
      "Epoch: 246, Train Acc: 0.7802197802197802\n",
      "Epoch: 247, Train Acc: 0.7802197802197802\n",
      "Epoch: 248, Train Acc: 0.7802197802197802\n",
      "Epoch: 249, Train Acc: 0.7802197802197802\n",
      "Epoch: 250, Train Acc: 0.7802197802197802\n",
      "Epoch: 251, Train Acc: 0.7912087912087912\n",
      "Epoch: 252, Train Acc: 0.7912087912087912\n",
      "Epoch: 253, Train Acc: 0.7912087912087912\n",
      "Epoch: 254, Train Acc: 0.7912087912087912\n",
      "Epoch: 255, Train Acc: 0.7912087912087912\n",
      "Epoch: 256, Train Acc: 0.7912087912087912\n",
      "Epoch: 257, Train Acc: 0.7912087912087912\n",
      "Epoch: 258, Train Acc: 0.7912087912087912\n",
      "Epoch: 259, Train Acc: 0.7912087912087912\n",
      "Epoch: 260, Train Acc: 0.7912087912087912\n",
      "Epoch: 261, Train Acc: 0.7912087912087912\n",
      "Epoch: 262, Train Acc: 0.7912087912087912\n",
      "Epoch: 263, Train Acc: 0.7912087912087912\n",
      "Epoch: 264, Train Acc: 0.7912087912087912\n",
      "Epoch: 265, Train Acc: 0.7912087912087912\n",
      "Epoch: 266, Train Acc: 0.7912087912087912\n",
      "Epoch: 267, Train Acc: 0.7912087912087912\n",
      "Epoch: 268, Train Acc: 0.7912087912087912\n",
      "Epoch: 269, Train Acc: 0.7912087912087912\n",
      "Epoch: 270, Train Acc: 0.7912087912087912\n",
      "Epoch: 271, Train Acc: 0.7912087912087912\n",
      "Epoch: 272, Train Acc: 0.7912087912087912\n",
      "Epoch: 273, Train Acc: 0.7912087912087912\n",
      "Epoch: 274, Train Acc: 0.7912087912087912\n",
      "Epoch: 275, Train Acc: 0.7912087912087912\n",
      "Epoch: 276, Train Acc: 0.7912087912087912\n",
      "Epoch: 277, Train Acc: 0.7912087912087912\n",
      "Epoch: 278, Train Acc: 0.7912087912087912\n",
      "Epoch: 279, Train Acc: 0.7912087912087912\n",
      "Epoch: 280, Train Acc: 0.7912087912087912\n",
      "Epoch: 281, Train Acc: 0.7912087912087912\n",
      "Epoch: 282, Train Acc: 0.7912087912087912\n",
      "Epoch: 283, Train Acc: 0.7912087912087912\n",
      "Epoch: 284, Train Acc: 0.7912087912087912\n",
      "Epoch: 285, Train Acc: 0.7912087912087912\n",
      "Epoch: 286, Train Acc: 0.8021978021978022\n",
      "Epoch: 287, Train Acc: 0.8021978021978022\n",
      "Epoch: 288, Train Acc: 0.8021978021978022\n",
      "Epoch: 289, Train Acc: 0.8021978021978022\n",
      "Epoch: 290, Train Acc: 0.8021978021978022\n",
      "Epoch: 291, Train Acc: 0.8021978021978022\n",
      "Epoch: 292, Train Acc: 0.8021978021978022\n",
      "Epoch: 293, Train Acc: 0.8021978021978022\n",
      "Epoch: 294, Train Acc: 0.8021978021978022\n",
      "Epoch: 295, Train Acc: 0.8021978021978022\n",
      "Epoch: 296, Train Acc: 0.8021978021978022\n",
      "Epoch: 297, Train Acc: 0.8021978021978022\n",
      "Epoch: 298, Train Acc: 0.8021978021978022\n",
      "Epoch: 299, Train Acc: 0.8021978021978022\n",
      "Epoch: 300, Train Acc: 0.8021978021978022\n",
      "Epoch: 301, Train Acc: 0.8021978021978022\n",
      "Epoch: 302, Train Acc: 0.8021978021978022\n",
      "Epoch: 303, Train Acc: 0.8021978021978022\n",
      "Epoch: 304, Train Acc: 0.8021978021978022\n",
      "Epoch: 305, Train Acc: 0.8021978021978022\n",
      "Epoch: 306, Train Acc: 0.8021978021978022\n",
      "Epoch: 307, Train Acc: 0.8021978021978022\n",
      "Epoch: 308, Train Acc: 0.8021978021978022\n",
      "Epoch: 309, Train Acc: 0.8021978021978022\n",
      "Epoch: 310, Train Acc: 0.8021978021978022\n",
      "Epoch: 311, Train Acc: 0.8021978021978022\n",
      "Epoch: 312, Train Acc: 0.8021978021978022\n",
      "Epoch: 313, Train Acc: 0.8021978021978022\n",
      "Epoch: 314, Train Acc: 0.8021978021978022\n",
      "Epoch: 315, Train Acc: 0.8021978021978022\n",
      "Epoch: 316, Train Acc: 0.8021978021978022\n",
      "Epoch: 317, Train Acc: 0.8021978021978022\n",
      "Epoch: 318, Train Acc: 0.8021978021978022\n",
      "Epoch: 319, Train Acc: 0.8021978021978022\n",
      "Epoch: 320, Train Acc: 0.8021978021978022\n",
      "Epoch: 321, Train Acc: 0.8021978021978022\n",
      "Epoch: 322, Train Acc: 0.8021978021978022\n",
      "Epoch: 323, Train Acc: 0.8021978021978022\n",
      "Epoch: 324, Train Acc: 0.8021978021978022\n",
      "Epoch: 325, Train Acc: 0.8021978021978022\n",
      "Epoch: 326, Train Acc: 0.8021978021978022\n",
      "Epoch: 327, Train Acc: 0.8021978021978022\n",
      "Epoch: 328, Train Acc: 0.8021978021978022\n",
      "Epoch: 329, Train Acc: 0.8021978021978022\n",
      "Epoch: 330, Train Acc: 0.8021978021978022\n",
      "Epoch: 331, Train Acc: 0.8021978021978022\n",
      "Epoch: 332, Train Acc: 0.8021978021978022\n",
      "Epoch: 333, Train Acc: 0.8021978021978022\n",
      "Epoch: 334, Train Acc: 0.8021978021978022\n",
      "Epoch: 335, Train Acc: 0.8021978021978022\n",
      "Epoch: 336, Train Acc: 0.8021978021978022\n",
      "Epoch: 337, Train Acc: 0.8021978021978022\n",
      "Epoch: 338, Train Acc: 0.8021978021978022\n",
      "Epoch: 339, Train Acc: 0.8021978021978022\n",
      "Epoch: 340, Train Acc: 0.8021978021978022\n",
      "Epoch: 341, Train Acc: 0.8021978021978022\n",
      "Epoch: 342, Train Acc: 0.8021978021978022\n",
      "Epoch: 343, Train Acc: 0.8021978021978022\n",
      "Epoch: 344, Train Acc: 0.8021978021978022\n",
      "Epoch: 345, Train Acc: 0.8021978021978022\n",
      "Epoch: 346, Train Acc: 0.8021978021978022\n",
      "Epoch: 347, Train Acc: 0.8021978021978022\n",
      "Epoch: 348, Train Acc: 0.8021978021978022\n",
      "Epoch: 349, Train Acc: 0.8021978021978022\n",
      "Epoch: 350, Train Acc: 0.8021978021978022\n",
      "Epoch: 351, Train Acc: 0.8021978021978022\n",
      "Epoch: 352, Train Acc: 0.8021978021978022\n",
      "Epoch: 353, Train Acc: 0.8021978021978022\n",
      "Epoch: 354, Train Acc: 0.8021978021978022\n",
      "Epoch: 355, Train Acc: 0.8021978021978022\n",
      "Epoch: 356, Train Acc: 0.8021978021978022\n",
      "Epoch: 357, Train Acc: 0.8021978021978022\n",
      "Epoch: 358, Train Acc: 0.8021978021978022\n",
      "Epoch: 359, Train Acc: 0.8021978021978022\n",
      "Epoch: 360, Train Acc: 0.8021978021978022\n",
      "Epoch: 361, Train Acc: 0.8021978021978022\n",
      "Epoch: 362, Train Acc: 0.8021978021978022\n",
      "Epoch: 363, Train Acc: 0.8021978021978022\n",
      "Epoch: 364, Train Acc: 0.8021978021978022\n",
      "Epoch: 365, Train Acc: 0.8021978021978022\n",
      "Epoch: 366, Train Acc: 0.8021978021978022\n",
      "Epoch: 367, Train Acc: 0.8021978021978022\n",
      "Epoch: 368, Train Acc: 0.8131868131868132\n",
      "Epoch: 369, Train Acc: 0.8131868131868132\n",
      "Epoch: 370, Train Acc: 0.8131868131868132\n",
      "Epoch: 371, Train Acc: 0.8131868131868132\n",
      "Epoch: 372, Train Acc: 0.8131868131868132\n",
      "Epoch: 373, Train Acc: 0.8131868131868132\n",
      "Epoch: 374, Train Acc: 0.8131868131868132\n",
      "Epoch: 375, Train Acc: 0.8131868131868132\n",
      "Epoch: 376, Train Acc: 0.8131868131868132\n",
      "Epoch: 377, Train Acc: 0.8131868131868132\n",
      "Epoch: 378, Train Acc: 0.8131868131868132\n",
      "Epoch: 379, Train Acc: 0.8131868131868132\n",
      "Epoch: 380, Train Acc: 0.8131868131868132\n",
      "Epoch: 381, Train Acc: 0.8131868131868132\n",
      "Epoch: 382, Train Acc: 0.8131868131868132\n",
      "Epoch: 383, Train Acc: 0.8131868131868132\n",
      "Epoch: 384, Train Acc: 0.8131868131868132\n",
      "Epoch: 385, Train Acc: 0.8131868131868132\n",
      "Epoch: 386, Train Acc: 0.8131868131868132\n",
      "Epoch: 387, Train Acc: 0.8131868131868132\n",
      "Epoch: 388, Train Acc: 0.8131868131868132\n",
      "Epoch: 389, Train Acc: 0.8241758241758241\n",
      "Epoch: 390, Train Acc: 0.8241758241758241\n",
      "Epoch: 391, Train Acc: 0.8241758241758241\n",
      "Epoch: 392, Train Acc: 0.8241758241758241\n",
      "Epoch: 393, Train Acc: 0.8241758241758241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 394, Train Acc: 0.8241758241758241\n",
      "Epoch: 395, Train Acc: 0.8241758241758241\n",
      "Epoch: 396, Train Acc: 0.8241758241758241\n",
      "Epoch: 397, Train Acc: 0.8241758241758241\n",
      "Epoch: 398, Train Acc: 0.8351648351648352\n",
      "Epoch: 399, Train Acc: 0.8351648351648352\n",
      "Epoch: 400, Train Acc: 0.8351648351648352\n",
      "Epoch: 401, Train Acc: 0.8351648351648352\n",
      "Epoch: 402, Train Acc: 0.8351648351648352\n",
      "Epoch: 403, Train Acc: 0.8351648351648352\n",
      "Epoch: 404, Train Acc: 0.8351648351648352\n",
      "Epoch: 405, Train Acc: 0.8351648351648352\n",
      "Epoch: 406, Train Acc: 0.8351648351648352\n",
      "Epoch: 407, Train Acc: 0.8351648351648352\n",
      "Epoch: 408, Train Acc: 0.8351648351648352\n",
      "Epoch: 409, Train Acc: 0.8351648351648352\n",
      "Epoch: 410, Train Acc: 0.8351648351648352\n",
      "Epoch: 411, Train Acc: 0.8351648351648352\n",
      "Epoch: 412, Train Acc: 0.8351648351648352\n",
      "Epoch: 413, Train Acc: 0.8351648351648352\n",
      "Epoch: 414, Train Acc: 0.8351648351648352\n",
      "Epoch: 415, Train Acc: 0.8351648351648352\n",
      "Epoch: 416, Train Acc: 0.8351648351648352\n",
      "Epoch: 417, Train Acc: 0.8351648351648352\n",
      "Epoch: 418, Train Acc: 0.8351648351648352\n",
      "Epoch: 419, Train Acc: 0.8351648351648352\n",
      "Epoch: 420, Train Acc: 0.8351648351648352\n",
      "Epoch: 421, Train Acc: 0.8351648351648352\n",
      "Epoch: 422, Train Acc: 0.8351648351648352\n",
      "Epoch: 423, Train Acc: 0.8351648351648352\n",
      "Epoch: 424, Train Acc: 0.8351648351648352\n",
      "Epoch: 425, Train Acc: 0.8351648351648352\n",
      "Epoch: 426, Train Acc: 0.8351648351648352\n",
      "Epoch: 427, Train Acc: 0.8351648351648352\n",
      "Epoch: 428, Train Acc: 0.8351648351648352\n",
      "Epoch: 429, Train Acc: 0.8351648351648352\n",
      "Epoch: 430, Train Acc: 0.8351648351648352\n",
      "Epoch: 431, Train Acc: 0.8351648351648352\n",
      "Epoch: 432, Train Acc: 0.8351648351648352\n",
      "Epoch: 433, Train Acc: 0.8351648351648352\n",
      "Epoch: 434, Train Acc: 0.8351648351648352\n",
      "Epoch: 435, Train Acc: 0.8351648351648352\n",
      "Epoch: 436, Train Acc: 0.8351648351648352\n",
      "Epoch: 437, Train Acc: 0.8351648351648352\n",
      "Epoch: 438, Train Acc: 0.8351648351648352\n",
      "Epoch: 439, Train Acc: 0.8351648351648352\n",
      "Epoch: 440, Train Acc: 0.8351648351648352\n",
      "Epoch: 441, Train Acc: 0.8351648351648352\n",
      "Epoch: 442, Train Acc: 0.8351648351648352\n",
      "Epoch: 443, Train Acc: 0.8351648351648352\n",
      "Epoch: 444, Train Acc: 0.8351648351648352\n",
      "Epoch: 445, Train Acc: 0.8351648351648352\n",
      "Epoch: 446, Train Acc: 0.8351648351648352\n",
      "Epoch: 447, Train Acc: 0.8351648351648352\n",
      "Epoch: 448, Train Acc: 0.8351648351648352\n",
      "Epoch: 449, Train Acc: 0.8351648351648352\n",
      "Epoch: 450, Train Acc: 0.8351648351648352\n",
      "Epoch: 451, Train Acc: 0.8351648351648352\n",
      "Epoch: 452, Train Acc: 0.8351648351648352\n",
      "Epoch: 453, Train Acc: 0.8351648351648352\n",
      "Epoch: 454, Train Acc: 0.8351648351648352\n",
      "Epoch: 455, Train Acc: 0.8351648351648352\n",
      "Epoch: 456, Train Acc: 0.8351648351648352\n",
      "Epoch: 457, Train Acc: 0.8351648351648352\n",
      "Epoch: 458, Train Acc: 0.8351648351648352\n",
      "Epoch: 459, Train Acc: 0.8351648351648352\n",
      "Epoch: 460, Train Acc: 0.8351648351648352\n",
      "Epoch: 461, Train Acc: 0.8351648351648352\n",
      "Epoch: 462, Train Acc: 0.8351648351648352\n",
      "Epoch: 463, Train Acc: 0.8351648351648352\n",
      "Epoch: 464, Train Acc: 0.8351648351648352\n",
      "Epoch: 465, Train Acc: 0.8351648351648352\n",
      "Epoch: 466, Train Acc: 0.8351648351648352\n",
      "Epoch: 467, Train Acc: 0.8461538461538461\n",
      "Epoch: 468, Train Acc: 0.8461538461538461\n",
      "Epoch: 469, Train Acc: 0.8461538461538461\n",
      "Epoch: 470, Train Acc: 0.8461538461538461\n",
      "Epoch: 471, Train Acc: 0.8461538461538461\n",
      "Epoch: 472, Train Acc: 0.8461538461538461\n",
      "Epoch: 473, Train Acc: 0.8461538461538461\n",
      "Epoch: 474, Train Acc: 0.8461538461538461\n",
      "Epoch: 475, Train Acc: 0.8461538461538461\n",
      "Epoch: 476, Train Acc: 0.8461538461538461\n",
      "Epoch: 477, Train Acc: 0.8461538461538461\n",
      "Epoch: 478, Train Acc: 0.8461538461538461\n",
      "Epoch: 479, Train Acc: 0.8461538461538461\n",
      "Epoch: 480, Train Acc: 0.8461538461538461\n",
      "Epoch: 481, Train Acc: 0.8461538461538461\n",
      "Epoch: 482, Train Acc: 0.8461538461538461\n",
      "Epoch: 483, Train Acc: 0.8461538461538461\n",
      "Epoch: 484, Train Acc: 0.8461538461538461\n",
      "Epoch: 485, Train Acc: 0.8461538461538461\n",
      "Epoch: 486, Train Acc: 0.8461538461538461\n",
      "Epoch: 487, Train Acc: 0.8461538461538461\n",
      "Epoch: 488, Train Acc: 0.8461538461538461\n",
      "Epoch: 489, Train Acc: 0.8461538461538461\n",
      "Epoch: 490, Train Acc: 0.8461538461538461\n",
      "Epoch: 491, Train Acc: 0.8461538461538461\n",
      "Epoch: 492, Train Acc: 0.8461538461538461\n",
      "Epoch: 493, Train Acc: 0.8461538461538461\n",
      "Epoch: 494, Train Acc: 0.8461538461538461\n",
      "Epoch: 495, Train Acc: 0.8461538461538461\n",
      "Epoch: 496, Train Acc: 0.8461538461538461\n",
      "Epoch: 497, Train Acc: 0.8461538461538461\n",
      "Epoch: 498, Train Acc: 0.8461538461538461\n",
      "Epoch: 499, Train Acc: 0.8461538461538461\n",
      "[['\"SRP161461\"' '\"SRR7817611\"' '\"male\"' '0' '0' '0' '0.8021863102912903'\n",
      "  '0.19781376421451569']\n",
      " ['\"SRP161461\"' '\"SRR7817612\"' '\"male\"' '0' '0' '0' '0.7929279208183289'\n",
      "  '0.20707210898399353']\n",
      " ['\"SRP161461\"' '\"SRR7817614\"' '\"male\"' '0' '0' '0' '0.8220331072807312'\n",
      "  '0.1779668629169464']\n",
      " ['\"SRP161461\"' '\"SRR7817615\"' '\"male\"' '0' '0' '0' '0.8096597790718079'\n",
      "  '0.19034023582935333']\n",
      " ['\"SRP161461\"' '\"SRR7817616\"' '\"male\"' '0' '0' '0' '0.8088151812553406'\n",
      "  '0.19118481874465942']\n",
      " ['\"SRP161461\"' '\"SRR7817623\"' '\"male\"' '0' '0' '0' '0.8263553380966187'\n",
      "  '0.17364467680454254']\n",
      " ['\"SRP161461\"' '\"SRR7817624\"' '\"male\"' '0' '0' '0' '0.8554989099502563'\n",
      "  '0.14450116455554962']\n",
      " ['\"SRP161461\"' '\"SRR7817625\"' '\"male\"' '0' '0' '0' '0.7745526432991028'\n",
      "  '0.22544734179973602']\n",
      " ['\"SRP161461\"' '\"SRR7817626\"' '\"male\"' '0' '0' '0' '0.7558118104934692'\n",
      "  '0.24418815970420837']\n",
      " ['\"SRP161461\"' '\"SRR7817627\"' '\"male\"' '0' '0' '0' '0.8065173029899597'\n",
      "  '0.19348271191120148']\n",
      " ['\"SRP161461\"' '\"SRR7817628\"' '\"male\"' '0' '0' '0' '0.8836095929145813'\n",
      "  '0.1163904070854187']\n",
      " ['\"SRP161461\"' '\"SRR7817635\"' '\"male\"' '0' '0' '0' '0.8638169169425964'\n",
      "  '0.13618312776088715']\n",
      " ['\"SRP161461\"' '\"SRR7817636\"' '\"male\"' '0' '0' '0' '0.8713758587837219'\n",
      "  '0.12862414121627808']\n",
      " ['\"SRP161461\"' '\"SRR7817637\"' '\"male\"' '0' '0' '0' '0.850403904914856'\n",
      "  '0.14959608018398285']\n",
      " ['\"SRP161461\"' '\"SRR7817638\"' '\"male\"' '0' '0' '0' '0.8269869089126587'\n",
      "  '0.17301303148269653']\n",
      " ['\"SRP161461\"' '\"SRR7817639\"' '\"male\"' '0' '0' '1' '0.48531055450439453'\n",
      "  '0.5146894454956055']\n",
      " ['\"SRP161461\"' '\"SRR7817640\"' '\"male\"' '0' '0' '0' '0.6325651407241821'\n",
      "  '0.3674348294734955']\n",
      " ['\"SRP161461\"' '\"SRR7817647\"' '\"male\"' '0' '0' '0' '0.8242757320404053'\n",
      "  '0.17572428286075592']\n",
      " ['\"SRP161461\"' '\"SRR7817648\"' '\"male\"' '0' '0' '0' '0.9083685278892517'\n",
      "  '0.0916314646601677']\n",
      " ['\"SRP161461\"' '\"SRR7817649\"' '\"male\"' '0' '0' '0' '0.8057824373245239'\n",
      "  '0.1942174881696701']\n",
      " ['\"SRP161461\"' '\"SRR7817650\"' '\"male\"' '0' '0' '0' '0.8695718050003052'\n",
      "  '0.13042820990085602']\n",
      " ['\"SRP161461\"' '\"SRR7817651\"' '\"male\"' '0' '0' '0' '0.7764928936958313'\n",
      "  '0.2235070914030075']\n",
      " ['\"SRP161461\"' '\"SRR7817652\"' '\"male\"' '0' '0' '0' '0.5984389781951904'\n",
      "  '0.40156102180480957']\n",
      " ['\"SRP161461\"' '\"SRR7817659\"' '\"male\"' '0' '0' '0' '0.8344973921775818'\n",
      "  '0.1655026078224182']\n",
      " ['\"SRP161461\"' '\"SRR7817660\"' '\"male\"' '0' '0' '0' '0.8126721978187561'\n",
      "  '0.18732783198356628']\n",
      " ['\"SRP161461\"' '\"SRR7817661\"' '\"male\"' '0' '0' '0' '0.901705801486969'\n",
      "  '0.09829419106245041']\n",
      " ['\"SRP161461\"' '\"SRR7817662\"' '\"male\"' '0' '0' '0' '0.8979415893554688'\n",
      "  '0.10205844789743423']\n",
      " ['\"SRP161461\"' '\"SRR7817663\"' '\"male\"' '0' '0' '0' '0.8075729012489319'\n",
      "  '0.1924271136522293']\n",
      " ['\"SRP161461\"' '\"SRR7817671\"' '\"male\"' '0' '0' '0' '0.9007667303085327'\n",
      "  '0.09923325479030609']\n",
      " ['\"SRP161461\"' '\"SRR7817672\"' '\"male\"' '0' '0' '0' '0.7306451797485352'\n",
      "  '0.26935485005378723']\n",
      " ['\"SRP161461\"' '\"SRR7817673\"' '\"male\"' '0' '0' '0' '0.9763131737709045'\n",
      "  '0.023686768487095833']\n",
      " ['\"SRP161461\"' '\"SRR7817674\"' '\"male\"' '0' '0' '0' '0.9637027978897095'\n",
      "  '0.03629717975854874']\n",
      " ['\"SRP161461\"' '\"SRR7817675\"' '\"male\"' '0' '0' '0' '0.8326611518859863'\n",
      "  '0.16733890771865845']\n",
      " ['\"SRP161461\"' '\"SRR7817676\"' '\"male\"' '0' '0' '0' '0.8281716108322144'\n",
      "  '0.17182840406894684']\n",
      " ['\"SRP161461\"' '\"SRR7817683\"' '\"male\"' '0' '0' '0' '0.8912046551704407'\n",
      "  '0.10879534482955933']\n",
      " ['\"SRP161461\"' '\"SRR7817684\"' '\"male\"' '0' '0' '0' '0.8852291107177734'\n",
      "  '0.11477088928222656']\n",
      " ['\"SRP161461\"' '\"SRR7817685\"' '\"male\"' '0' '0' '0' '0.8744361996650696'\n",
      "  '0.125563845038414']\n",
      " ['\"SRP161461\"' '\"SRR7817686\"' '\"male\"' '0' '0' '0' '0.9172182083129883'\n",
      "  '0.08278176933526993']\n",
      " ['\"SRP161461\"' '\"SRR7817695\"' '\"male\"' '0' '0' '0' '0.867252767086029'\n",
      "  '0.13274729251861572']\n",
      " ['\"SRP161461\"' '\"SRR7817696\"' '\"male\"' '0' '0' '0' '0.8183898329734802'\n",
      "  '0.1816101223230362']\n",
      " ['\"SRP161461\"' '\"SRR7817697\"' '\"male\"' '0' '0' '0' '0.8762317299842834'\n",
      "  '0.12376825511455536']\n",
      " ['\"SRP161461\"' '\"SRR7817698\"' '\"male\"' '0' '0' '0' '0.6971673369407654'\n",
      "  '0.302832692861557']\n",
      " ['\"SRP161461\"' '\"SRR7817699\"' '\"male\"' '0' '0' '0' '0.8870653510093689'\n",
      "  '0.11293470114469528']\n",
      " ['\"SRP161461\"' '\"SRR7817700\"' '\"male\"' '0' '0' '0' '0.9084289073944092'\n",
      "  '0.09157109260559082']\n",
      " ['\"SRP161461\"' '\"SRR7817613\"' '\"male\"' '0' '0' '0' '0.7847548723220825'\n",
      "  '0.21524512767791748']\n",
      " ['\"SRP161461\"' '\"SRR7817664\"' '\"male\"' '0' '0' '0' '0.8721078038215637'\n",
      "  '0.1278921514749527']\n",
      " ['\"SRP161461\"' '\"SRR7817687\"' '\"male\"' '0' '0' '0' '0.8531908988952637'\n",
      "  '0.14680908620357513']\n",
      " ['\"SRP161461\"' '\"SRR7817688\"' '\"male\"' '0' '0' '0' '0.8133233189582825'\n",
      "  '0.18667668104171753']\n",
      " ['\"SRP049440\"' '\"SRR1636586\"' '\"female\"' '0' '0' '1'\n",
      "  '0.3909463882446289' '0.6090536117553711']\n",
      " ['\"SRP049440\"' '\"SRR1636587\"' '\"female\"' '0' '0' '0'\n",
      "  '0.7435298562049866' '0.25647008419036865']\n",
      " ['\"SRP049440\"' '\"SRR1636591\"' '\"female\"' '0' '0' '0' '0.569720447063446'\n",
      "  '0.43027961254119873']\n",
      " ['\"SRP049440\"' '\"SRR1636592\"' '\"female\"' '0' '0' '0'\n",
      "  '0.5651910901069641' '0.4348089098930359']\n",
      " ['\"SRP049440\"' '\"SRR1636593\"' '\"female\"' '0' '0' '0'\n",
      "  '0.5199289917945862' '0.4800710082054138']\n",
      " ['\"SRP049440\"' '\"SRR1636594\"' '\"female\"' '0' '0' '0' '0.675369918346405'\n",
      "  '0.32463008165359497']\n",
      " ['\"SRP049440\"' '\"SRR1636606\"' '\"female\"' '0.03' '1' '1'\n",
      "  '0.31034788489341736' '0.689652144908905']\n",
      " ['\"SRP049440\"' '\"SRR1636607\"' '\"female\"' '0.03' '1' '1'\n",
      "  '0.4136965274810791' '0.5863034725189209']\n",
      " ['\"SRP049440\"' '\"SRR1636608\"' '\"female\"' '0.03' '1' '1'\n",
      "  '0.04032158479094505' '0.9596784710884094']\n",
      " ['\"SRP049440\"' '\"SRR1636588\"' '\"female\"' '0' '0' '1'\n",
      "  '0.3757118284702301' '0.6242881417274475']\n",
      " ['\"SRP049440\"' '\"SRR1636589\"' '\"female\"' '0' '0' '1'\n",
      "  '0.37642011046409607' '0.6235798597335815']\n",
      " ['\"SRP049440\"' '\"SRR1636590\"' '\"female\"' '0' '0' '0'\n",
      "  '0.5580757856369019' '0.44192418456077576']\n",
      " ['\"SRP049440\"' '\"SRR1636609\"' '\"female\"' '0.03' '1' '1'\n",
      "  '0.07616271823644638' '0.9238373041152954']\n",
      " ['\"SRP049440\"' '\"SRR1636610\"' '\"female\"' '0.03' '1' '1'\n",
      "  '0.19676747918128967' '0.8032325506210327']\n",
      " ['\"SRP049440\"' '\"SRR1636611\"' '\"female\"' '0.03' '1' '1'\n",
      "  '0.40565046668052673' '0.5943495631217957']\n",
      " ['\"SRP049440\"' '\"SRR1636612\"' '\"female\"' '0.03' '1' '1'\n",
      "  '0.3318841755390167' '0.6681158542633057']\n",
      " ['\"SRP049440\"' '\"SRR1636613\"' '\"female\"' '0.03' '1' '1'\n",
      "  '0.14058107137680054' '0.8594189882278442']\n",
      " ['\"SRP049440\"' '\"SRR1636614\"' '\"female\"' '0.03' '1' '1'\n",
      "  '0.1488390415906906' '0.8511609435081482']\n",
      " ['\"SRP049440\"' '\"SRR1636615\"' '\"female\"' '0.03' '1' '1'\n",
      "  '0.24258899688720703' '0.757411003112793']\n",
      " ['\"SRP049440\"' '\"SRR1636616\"' '\"female\"' '0.03' '1' '1'\n",
      "  '0.19763758778572083' '0.8023624420166016']\n",
      " ['\"SRP075814\"' '\"SRR3593523\"' '\"female\"' '0' '0' '0'\n",
      "  '0.6458660960197449' '0.3541339933872223']\n",
      " ['\"SRP075814\"' '\"SRR3593524\"' '\"female\"' '0' '0' '0'\n",
      "  '0.6781817078590393' '0.3218182921409607']\n",
      " ['\"SRP075814\"' '\"SRR3593525\"' '\"female\"' '0' '0' '0' '0.634871244430542'\n",
      "  '0.365128755569458']\n",
      " ['\"SRP075814\"' '\"SRR3593527\"' '\"female\"' '0' '0' '0'\n",
      "  '0.5843068957328796' '0.41569316387176514']\n",
      " ['\"SRP075814\"' '\"SRR3593526\"' '\"female\"' '0' '0' '0'\n",
      "  '0.6785315871238708' '0.32146844267845154']\n",
      " ['\"SRP075814\"' '\"SRR3593534\"' '\"female\"' '0.03' '1' '0'\n",
      "  '0.5399811863899231' '0.4600187838077545']\n",
      " ['\"SRP075814\"' '\"SRR3593535\"' '\"female\"' '0.03' '1' '0'\n",
      "  '0.6690500974655151' '0.3309498727321625']\n",
      " ['\"SRP075814\"' '\"SRR3593536\"' '\"female\"' '0.03' '1' '1'\n",
      "  '0.40329307317733765' '0.5967069864273071']\n",
      " ['\"SRP075814\"' '\"SRR3593537\"' '\"female\"' '0.03' '1' '0'\n",
      "  '0.5827077031135559' '0.41729220747947693']\n",
      " ['\"SRP075814\"' '\"SRR3593538\"' '\"female\"' '0.03' '1' '0'\n",
      "  '0.6309256553649902' '0.3690743148326874']\n",
      " ['\"SRP075814\"' '\"SRR3593539\"' '\"female\"' '0.03' '1' '0'\n",
      "  '0.7313348054885864' '0.2686651051044464']\n",
      " ['\"SRP090688\"' '\"SRR4317608\"' '\"male\"' '0' '0' '0' '0.636421799659729'\n",
      "  '0.36357825994491577']\n",
      " ['\"SRP090688\"' '\"SRR4317619\"' '\"male\"' '0.03' '1' '0'\n",
      "  '0.7586521506309509' '0.24134789407253265']\n",
      " ['\"SRP090688\"' '\"SRR4317620\"' '\"male\"' '0.03' '1' '0'\n",
      "  '0.5732111930847168' '0.42678871750831604']\n",
      " ['\"SRP090688\"' '\"SRR4317621\"' '\"male\"' '0.03' '1' '0'\n",
      "  '0.6121208071708679' '0.3878791332244873']\n",
      " ['\"SRP090688\"' '\"SRR4317622\"' '\"male\"' '0.03' '1' '0'\n",
      "  '0.5135089159011841' '0.48649105429649353']\n",
      " ['\"SRP090688\"' '\"SRR4317623\"' '\"male\"' '0.03' '1' '1'\n",
      "  '0.4651244282722473' '0.5348755121231079']\n",
      " ['\"SRP090688\"' '\"SRR4317624\"' '\"male\"' '0.03' '1' '1'\n",
      "  '0.4898149371147156' '0.5101850032806396']\n",
      " ['\"SRP090688\"' '\"SRR4317607\"' '\"male\"' '0' '0' '0' '0.6422272324562073'\n",
      "  '0.3577727973461151']\n",
      " ['\"SRP090688\"' '\"SRR4317609\"' '\"male\"' '0' '0' '1' '0.32691025733947754'\n",
      "  '0.6730897426605225']\n",
      " ['\"SRP090688\"' '\"SRR4317610\"' '\"male\"' '0' '0' '0' '0.7032058835029602'\n",
      "  '0.29679402709007263']\n",
      " ['\"SRP090688\"' '\"SRR4317611\"' '\"male\"' '0' '0' '0' '0.7629650235176086'\n",
      "  '0.23703502118587494']\n",
      " ['\"SRP090688\"' '\"SRR4317612\"' '\"male\"' '0' '0' '0' '0.6071813702583313'\n",
      "  '0.3928186595439911']]\n",
      "ari: 0.428735482247928\n",
      "test_ari: 0.428735482247928\n",
      "model saved as /mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/output/tuned_pytorch_tcdd_model0.03vs0_time_cross.pt\n"
     ]
    }
   ],
   "source": [
    "data_list = build_reactome_graph_datalist(edge_v1, edge_v2, node_features_fn, graph_targets_fn, project_id_fn, sample_id_fn, gender_fn, dose_fn)\n",
    "print(len(data_list))\n",
    "# retrain model for fine tuning transfer learning\n",
    "train_data_list = data_list  # all data\n",
    "print(len(train_data_list))\n",
    "print(f'Number of training graphs: {len(train_data_list)}')\n",
    "train_data_loader = build_reactome_graph_loader(train_data_list, BATCH_SIZE)\n",
    "for epoch in range(EPOCHS):\n",
    "    train(train_data_loader, device)\n",
    "    train_acc = train(train_data_loader, device)\n",
    "    print(f'Epoch: {epoch}, Train Acc: {train_acc}')\n",
    "    if train_acc == 1.0:\n",
    "        break\n",
    "\n",
    "final_ari = test(train_data_loader, device)\n",
    "print(f'test_ari: {final_ari}')\n",
    "\n",
    "model_save_name = f'tuned_pytorch_tcdd_model0.03vs0_time_cross.pt'\n",
    "path = f'/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/tcdd/output/{model_save_name}'\n",
    "torch.save(model.state_dict(), path)\n",
    "print(f'model saved as {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d1a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
