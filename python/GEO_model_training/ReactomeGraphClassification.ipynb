{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fb6a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import sklearn\n",
    "import random\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import Javascript\n",
    "from torch.nn import Linear\n",
    "from sklearn import preprocessing\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GraphConv, global_mean_pool\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random.seed = 88888888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac89e6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = cuda0 = torch.device('cuda:0')\n",
    "cpu = torch.device('cpu')\n",
    "\n",
    "edges_fn = '/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/GEO_model_training/input/edges.txt'\n",
    "node_features_fn = '/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/GEO_model_training/input/node_features.txt'\n",
    "graph_targets_fn = '/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/GEO_model_training/input/graph_targets.txt'\n",
    "\n",
    "# magic numbers\n",
    "INPUT_CHANNELS = 1\n",
    "OUTPUT_CHANNELS = 26\n",
    "HIDDEN_CHANNELS = 64\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 500 #set this to 200 - 2000\n",
    "BENCHMARKING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a6b1090",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/GEO_model_training/input/node_features.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m feature_v \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_features_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(feature_v)\n",
      "File \u001b[0;32m/mnt/ufs18/nodr/home/naultran/miniforge-envs/tox_reticula/lib/python3.9/site-packages/numpy/lib/npyio.py:1373\u001b[0m, in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1371\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1373\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m            \u001b[49m\u001b[43munpack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munpack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[0;32m/mnt/ufs18/nodr/home/naultran/miniforge-envs/tox_reticula/lib/python3.9/site-packages/numpy/lib/npyio.py:992\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[1;32m    990\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fname)\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 992\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datasource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    994\u001b[0m         encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fh, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/mnt/ufs18/nodr/home/naultran/miniforge-envs/tox_reticula/lib/python3.9/site-packages/numpy/lib/_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m ds \u001b[38;5;241m=\u001b[39m DataSource(destpath)\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/ufs18/nodr/home/naultran/miniforge-envs/tox_reticula/lib/python3.9/site-packages/numpy/lib/_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    531\u001b[0m                               encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: /mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/GEO_model_training/input/node_features.txt not found."
     ]
    }
   ],
   "source": [
    "feature_v = numpy.loadtxt(node_features_fn)\n",
    "print(feature_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f67fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_reactome_graph(edges_fn, node_features_fn):\n",
    "    edge_v1 = []\n",
    "    edge_v2 = []\n",
    "\n",
    "    for line in open(edges_fn, 'r'):\n",
    "        data = line.split()\n",
    "        node1 = int(data[0]) - 1 #subtracting to convert R idx to python idx\n",
    "        node2 = int(data[1]) - 1 # \" \"\n",
    "        edge_v1.append( node1 )\n",
    "        edge_v2.append( node2 )\n",
    "\n",
    "    return edge_v1, edge_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc4de01",
   "metadata": {},
   "outputs": [],
   "source": [
    "(edge_v1, edge_v2) = read_reactome_graph(edges_fn, node_features_fn)\n",
    "print(edge_v1)\n",
    "print(edge_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe57fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_scratch_loader(batch_size):\n",
    "  dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n",
    "  data_list = []\n",
    "  for graph_obj in dataset:\n",
    "    x = torch.tensor(graph_obj.x[:,1],dtype=torch.float)\n",
    "    x = x.unsqueeze(1)\n",
    "    y = graph_obj.y\n",
    "    edge_index = graph_obj.edge_index\n",
    "    data_list.append(Data(x = x, y = y, edge_index = edge_index))\n",
    "\n",
    "  loader = DataLoader(data_list,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "  return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73153e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_reactome_graph_datalist(edge_v1, edge_v2, node_features_fn, graph_targets_fn):\n",
    "    edge_index = torch.tensor([edge_v1, edge_v2], dtype = torch.long)\n",
    "    feature_v = numpy.loadtxt(node_features_fn)\n",
    "    target_v = numpy.loadtxt(graph_targets_fn,dtype=str,delimiter=\",\")\n",
    "    \n",
    "    target_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "    target_v = target_encoder.fit_transform(target_v)\n",
    "    \n",
    "    print(len(feature_v))\n",
    "    print(len(target_v))\n",
    "\n",
    "    data_list = []\n",
    "    for row_idx in range(len(feature_v)):\n",
    "      features = feature_v[row_idx,:]\n",
    "      x = torch.tensor(features,dtype=torch.float)\n",
    "      x = x.unsqueeze(1)\n",
    "      y = torch.tensor([target_v[row_idx]])\n",
    "      data_list.append(Data(x = x, y = y, edge_index = edge_index))\n",
    "\n",
    "    return data_list\n",
    "\n",
    "def build_reactome_graph_loader(data_list,batch_size):\n",
    "\n",
    "    loader = DataLoader(data_list,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d5fbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.conv1 = GraphConv(INPUT_CHANNELS, hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels,hidden_channels)\n",
    "        self.conv3 = GraphConv(hidden_channels,hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, OUTPUT_CHANNELS)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_weight=None):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909aff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN(hidden_channels=HIDDEN_CHANNELS)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(loader,device):\n",
    "  model.train()\n",
    "\n",
    "  for batch in loader:  # Iterate in batches over the training dataset.\n",
    "    x = batch.x.to(device)\n",
    "    e = batch.edge_index.to(device)\n",
    "    b = batch.batch.to(device)\n",
    "    y = batch.y.to(device)\n",
    "    \n",
    "    out = model(x, e, b)  # Perform a single forward pass.\n",
    "    \n",
    "    loss = criterion(out, y)  # Compute the loss.\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader,device):\n",
    "  model.eval()\n",
    "\n",
    "  correct = 0\n",
    "  for batch in loader:  # Iterate in batches over the training/test dataset.\n",
    "    x = batch.x.to(device)\n",
    "    e = batch.edge_index.to(device)\n",
    "    b = batch.batch.to(device)\n",
    "    y = batch.y.to(device)\n",
    "    out = model(x, e, b)  # Perform a single forward pass.\n",
    "    loss = criterion(out, y)  # Compute the loss.\n",
    "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "    correct += int((pred == y).sum())  # Check against ground-truth labels.\n",
    "  return correct / len(loader.dataset)  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22d97a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_str = ''\n",
    "if(BENCHMARKING):\n",
    "\n",
    "  test_b_sizes = [1,8,16,32,64,128]\n",
    "\n",
    "  for test_b_size in test_b_sizes:\n",
    "    print(f'Executing training routine with batch size = {test_b_size}')\n",
    "    data_list = build_reactome_graph_datalist(edge_v1, edge_v2, node_features_fn, graph_targets_fn)\n",
    "    test_batch_size_data_loader = build_reactome_graph_loader(data_list,test_b_size)\n",
    "  \n",
    "    start = time.time()\n",
    "    train(test_batch_size_data_loader,device)\n",
    "    end = time.time()\n",
    "    training_time = end - start\n",
    "\n",
    "    start = time.time()\n",
    "    train_acc = test(test_batch_size_data_loader,device)\n",
    "    end = time.time()\n",
    "    test_time = end - start\n",
    "\n",
    "    acc_str += f'{train_acc:.4f}\\n'\n",
    "    print(f'Batch Size: {test_b_size}')\n",
    "    print(f'Training Time: {training_time}')\n",
    "    print(f'Test Time: {test_time}')\n",
    "    print(f'Accuracy: {train_acc}')\n",
    "    BENCHMARKING = False\n",
    "else:\n",
    "  #data_loader = build_scratch_loader(BATCH_SIZE) # testing\n",
    "  data_list = build_reactome_graph_datalist(edge_v1, edge_v2, node_features_fn, graph_targets_fn)\n",
    "  random.shuffle(data_list)\n",
    "#   print(data_list)\n",
    "\n",
    "  BENCHMARKING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b0fbe9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(BENCHMARKING):\n",
    "  fold_size = 911\n",
    "  fold = 'full_dataset'\n",
    "#   >>> train =              z[:fold_size * (fold - 1)] +         z[fold_size * fold:]\n",
    "#   train_data_list = data_list[:fold_size * (fold - 1)] + data_list[fold_size * fold:]\n",
    "  #>>> test =              z[fold_size * (fold - 1):fold_size * fold]\n",
    "  #test_data_list = data_list[fold_size * (fold - 1):fold_size * fold]\n",
    "  train_data_list = data_list\n",
    "\n",
    "  print(f'Number of training graphs: {len(train_data_list)}')\n",
    "  #print(f'Number of test graphs: {len(test_data_list)}')\n",
    "  train_data_loader = build_reactome_graph_loader(train_data_list,BATCH_SIZE)\n",
    "  #test_data_loader = build_reactome_graph_loader(test_data_list,BATCH_SIZE)\n",
    "  for epoch in range(EPOCHS):\n",
    "    train(train_data_loader,device)\n",
    "    train_acc = test(train_data_loader,device)\n",
    "    #test_acc = test(test_data_loader,device) \n",
    "    acc_str += f'{train_acc:.4f}'#',{test_acc:.4f}\\n'\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}')#', Test Acc: {test_acc:.4f}')\n",
    "\n",
    "  training_acc_fn = F\"graph_classification_acc_rewired10_{fold}.txt\"\n",
    "  path = F\"/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/GEO_model_training/GNN/{training_acc_fn}\"\n",
    "  with open(path, 'w') as writefile:\n",
    "      writefile.write(acc_str)\n",
    "  model_save_name = F\"trained_pytorch_model_rewired10_fold_{fold}.pt\"\n",
    "  path = F\"/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/GEO_model_training/GNN/{model_save_name}\" \n",
    "  torch.save(model.state_dict(), path)\n",
    "  print(F\"model saved as {path}\")\n",
    "  # real network gets to 0.8417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cad519",
   "metadata": {},
   "outputs": [],
   "source": [
    "DRAWING = True\n",
    "if(DRAWING):\n",
    "  import networkx as nx\n",
    "  import numpy as np\n",
    "  from torch_geometric.utils import to_networkx\n",
    "\n",
    "\n",
    "  def draw_molecule(g, edge_mask=None, draw_edge_labels=False):\n",
    "      g = g.copy().to_undirected()\n",
    "      node_labels = {}\n",
    "      for u, data in g.nodes(data=True):\n",
    "          node_labels[u] = data['name']\n",
    "      pos = nx.spring_layout(g)\n",
    "      if edge_mask is None:\n",
    "          edge_color = 'black'\n",
    "          widths = None\n",
    "      else:\n",
    "          edge_color = [edge_mask[(u, v)] for u, v in g.edges()]\n",
    "          widths = [x * 10 for x in edge_color]\n",
    "      nx.draw(g, pos=pos, labels=node_labels, width=widths,\n",
    "              edge_color=edge_color, edge_cmap=plt.cm.Blues,\n",
    "              node_color='azure')\n",
    "      \n",
    "      if draw_edge_labels and edge_mask is not None:\n",
    "          edge_labels = {k: ('%.2f' % v) for k, v in edge_mask.items()}    \n",
    "          nx.draw_networkx_edge_labels(g, pos, edge_labels=edge_labels,\n",
    "                                      font_color='red')\n",
    "      plt.show()\n",
    "\n",
    "\n",
    "  def to_molecule(data):\n",
    "      g = to_networkx(data, node_attrs=['x'])\n",
    "      print('g',g)\n",
    "      for u, data in g.nodes(data=True):\n",
    "          data['name'] = data['x']\n",
    "          del data['x']\n",
    "      print(data,g)\n",
    "      return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819afc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DRAWING):\n",
    "  from captum.attr import Saliency, IntegratedGradients\n",
    "\n",
    "  def model_forward(edge_mask, data):\n",
    "      batch = torch.zeros(data.x.shape[0], dtype=int).to(device)\n",
    "      out = model(data.x,\n",
    "                  data.edge_index, \n",
    "                  batch,\n",
    "                  edge_mask)\n",
    "      return out\n",
    "\n",
    "\n",
    "  def explain(method, data, target=0):\n",
    "      input_mask = torch.ones(data.edge_index.shape[1]).requires_grad_(True).to(device)\n",
    "      if method == 'ig':\n",
    "          ig = IntegratedGradients(model_forward)\n",
    "          mask = ig.attribute(input_mask,target=target,\n",
    "                              additional_forward_args=(data,),\n",
    "                              internal_batch_size=data.edge_index.shape[1])\n",
    "      elif method == 'saliency':\n",
    "          saliency = Saliency(model_forward)\n",
    "          mask = saliency.attribute(input_mask, target=target,\n",
    "                                    additional_forward_args=(data,))\n",
    "      else:\n",
    "          raise Exception('Unknown explanation method')\n",
    "\n",
    "      edge_mask = np.abs(mask.cpu().detach().numpy())\n",
    "      if edge_mask.max() > 0:  # avoid division by zero\n",
    "          edge_mask = edge_mask / edge_mask.max()\n",
    "      return edge_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299e05ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(edge_v1, edge_v2, node_features_fn, graph_targets_fn,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9909bcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DRAWING):\n",
    "    data_list = build_reactome_graph_datalist(edge_v1, edge_v2, node_features_fn, graph_targets_fn)\n",
    "    data_loader = build_reactome_graph_loader(data_list, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cd33c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DRAWING):\n",
    "  model = GNN(hidden_channels=HIDDEN_CHANNELS)\n",
    "  model = model.to(device)\n",
    "\n",
    "  model_save_name = 'trained_pytorch_model_rewired10_fold_full_dataset.pt'\n",
    "  path = F\"/mnt/home/yuankeji/RanceLab/reticula_new/reticula/data/GEO_model_training/GNN/{model_save_name}\" \n",
    "  model.load_state_dict(torch.load(path))\n",
    "  model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dca289",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DRAWING):\n",
    "  d = data_loader.dataset[0]\n",
    "  d.edge_index.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423859a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_loader.dataset[0])\n",
    "print(data_list)\n",
    "mol = to_molecule(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd01a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DRAWING):\n",
    "  import random\n",
    "  from collections import defaultdict\n",
    "\n",
    "  def aggregate_edge_directions(edge_mask, data):\n",
    "      edge_mask_dict = defaultdict(float)\n",
    "      for val, u, v in list(zip(edge_mask, *data.edge_index)):\n",
    "          u, v = u.item(), v.item()\n",
    "          if u > v:\n",
    "              u, v = v, u\n",
    "          edge_mask_dict[(u, v)] += val\n",
    "      return edge_mask_dict\n",
    "      \n",
    "  data = data_loader.dataset[0]\n",
    "  mol = to_molecule(data) # 'float' object has no attribute 'index'\n",
    "\n",
    "  for title, method in [('Integrated Gradients', 'ig'), ('Saliency', 'saliency')]:\n",
    "      data.to(device)\n",
    "      edge_mask = explain(method, data, target=0)\n",
    "      edge_mask_dict = aggregate_edge_directions(edge_mask, data)\n",
    "      plt.figure(figsize=(100, 50))\n",
    "      plt.title(title)\n",
    "      draw_molecule(mol, edge_mask_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95e4cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1240e210",
   "metadata": {},
   "outputs": [],
   "source": [
    "  def explain(method, data, target=0):\n",
    "      input_mask = torch.ones(data.edge_index.shape[1]).requires_grad_(True).to(device)\n",
    "      print('input_mask', input_mask)\n",
    "      if method == 'ig':\n",
    "          ig = IntegratedGradients(model_forward)\n",
    "          print('ig=', ig)\n",
    "          mask = ig.attribute(input_mask,target=target,\n",
    "                              additional_forward_args=(data,),\n",
    "                              internal_batch_size=data.edge_index.shape[1])\n",
    "          print('ig_mask', mask)\n",
    "      elif method == 'saliency':\n",
    "          saliency = Saliency(model_forward)\n",
    "          print('saliency=', saliency)\n",
    "          mask = saliency.attribute(input_mask, target=target,\n",
    "                                    additional_forward_args=(data,))\n",
    "          print('saliency_mask', mask)\n",
    "      else:\n",
    "          raise Exception('Unknown explanation method')\n",
    "\n",
    "      edge_mask = np.abs(mask.cpu().detach().numpy())\n",
    "      print('edge_mask', edge_mask)\n",
    "      if edge_mask.max() > 0:  # avoid division by zero\n",
    "          edge_mask = edge_mask / edge_mask.max()\n",
    "      return edge_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1387274",
   "metadata": {},
   "outputs": [],
   "source": [
    "  def aggregate_edge_directions(edge_mask, data):\n",
    "      edge_mask_dict = defaultdict(float)\n",
    "#       print('edge_mask_dict', edge_mask_dict)\n",
    "      for val, u, v in list(zip(edge_mask, *data.edge_index)):\n",
    "#           print(\"-----------\")\n",
    "#           print(val,u,v)\n",
    "#           print(\"-----------\")\n",
    "          u, v = u.item(), v.item()\n",
    "#           print(u,v)\n",
    "          if u > v:\n",
    "              u, v = v, u\n",
    "          edge_mask_dict[(u, v)] += val\n",
    "#           print('*****')\n",
    "#           print(edge_mask_dict)\n",
    "      return edge_mask_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed1912",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mol.nodes(data=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090425ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "  def draw_molecule(g, edge_mask=None, draw_edge_labels=False):\n",
    "      g = g.copy().to_undirected()\n",
    "      node_labels = {}\n",
    "      for u, data in g.nodes(data=True):\n",
    "          node_labels[u] = data['name']\n",
    "      pos = nx.spring_layout(g)\n",
    "      print(pos)\n",
    "      if edge_mask is None:\n",
    "          edge_color = 'black'\n",
    "          widths = None\n",
    "      else:\n",
    "          edge_color = [edge_mask[(u, v)] for u, v in g.edges()]\n",
    "          widths = [x * 10 for x in edge_color]\n",
    "      print(\"---------------\")\n",
    "      print('edge_color', edge_color)\n",
    "      print(\"---------------\")\n",
    "      print('widths', widths)\n",
    "      nx.draw(g, pos=pos, labels=node_labels, width=widths,\n",
    "              edge_color=edge_color, edge_cmap=plt.cm.Blues,\n",
    "              node_color='azure')\n",
    "      \n",
    "      if draw_edge_labels and edge_mask is not None:\n",
    "          edge_labels = {k: ('%.2f' % v) for k, v in edge_mask.items()}    \n",
    "          nx.draw_networkx_edge_labels(g, pos, edge_labels=edge_labels,\n",
    "                                      font_color='red')\n",
    "      plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410e889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    for title, method in [('Integrated Gradients', 'ig'), ('Saliency', 'saliency')]:\n",
    "      data.to(device)\n",
    "      edge_mask = explain(method, data, target=0)\n",
    "      print('edge_mask', edge_mask)\n",
    "      edge_mask_dict = aggregate_edge_directions(edge_mask, data)\n",
    "      plt.figure(figsize=(100, 50))\n",
    "      plt.title(title)\n",
    "      draw_molecule(mol, edge_mask_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
